# glm in R
The `glm` function fit a GLM model using the maximum likelihood method. For ordinary usage, the call is the same as for lm, except for one extra argument, family. The default value of family is gaussian, for each family we can specify a link function
* In the Poisson regression case the call looks like `glm(y~x, family = poisson)`
* In the logistic regression case, for example, the call looks like `glm(y~x, family = binomial)`

### Binomial example
```R
logitout <- glm(yesVote ~ . , data = chileElection, family = binomial)
```
The summary here prints out:
* Deviance Residuals, with the min, max and the quantiles
* Coefficients, the usual table with the estimations and significances.
  * Estimate 
  * Std. Error 
  * z value 
  * Pr(>|z|)
* A note `Dispersion parameter for binomial family taken to be 1` 
* Null deviance and the DoF, similar to R-Squared
* Residual deviance and the DoF, similar to the F-Statistic
* Number of observations deleted `54 observations deleted due to missingness`
* AIC
* Number of Fisher Scoring iterations

#### Link
As a default R uses the canonical link, for the binomial we check `binomial()$link` which returns `"logit"`. The link can be changed - see ?family

#### Variance 
Specifying the family implies the specification of the variance function:

```R
binomial()$variance

function (mu)
mu * (1 - mu)
<bytecode: 0x000001ee75be2310>
<environment: 0x000001ee764122d8>
```

#### Names
Specifying the family and the link implies a number of relationships:

```R
names(binomial())

[1] "family" "link" "linkfun" "linkinv" "variance"
[6] "dev.resids" "aic" "mu.eta" "initialize" "validmu"
[11] "valideta" "simulate"
```

---

## Fitting issues for the logistisc regression
We should note that, if there exists some $\beta^{*}$ such that $X\beta > 0 \Rightarrow y_{i}=1$ and $X\beta < 0 \Rightarrow y_{i}=0$, for all observations, then the MLE is not unique. Which is usually what happens when we have binary response variable.

Such data is said to be separable. There are many ways to proceed with the estimations. 

This, and similar numeric issues related to estimated probabilities near 0 or 1, will return a warning in R.

```R
Warning messages:
1: glm.fit: algorithm did not converge
2: glm.fit: fitted probabilities numerically 0 or 1 occurred
```

When this happens, the model is still “fit”, but there are consequences, namely, the estimated coefficients are highly suspect.
* This is an issue when then trying to interpret the model.
* However it could be useful for creating a classifier

---

## Inference for model parameters
R prints a summary with some information on the regression parameter estimates

The assumptions on which a generalized linear model is constructed allow us to specify what is the asymptotic distribution of the random vector $\hat{\beta}$ through the theory of MLE. 
* The estimators are normally distributed, unbiased and with the lowest possible variance. 
* We assume that the randomness of Y comes only from $(Y |X_{1} = x_{1},..., X_{p−1} = x_{p−1})$ and not from the predictors.

We rewrite the relationship between the expected value of $\mu$ and the linear predictor in matrix form:

$$\eta = g(\mu) = X\beta$$
* $\beta$ is a vector of parameters that we wish to estimate: we estimate them using an algorithm that maximizes the likelihood
* so the beta parameters enjoy optimal (approximate) properties of MLEs

There is an important difference between the inference results for the Gaussian linear model
and for glm:
* In Gaussian linear model the inference is exact. 
  * This is due to the nice properties of the normal, least squares estimation, and linearity. As a consequence, the distributions of the coefficients are perfectly known assuming that the assumptions hold.
* In generalized linear models the inference is **asymptotic**. 
  * This means that the distributions of the coefficients are unknown except for large sample sizes n, for which we have approximations. 
  * Keep in mind that in GLM-like models the relationsip between $\mu(x)$ and $x$ is non-linear: we fit a more complex model and this hinders our ability to make inference.

We can show that:

$$\hat{\beta} \thicksim \mathcal{N}(\beta, \mathcal{I}(\beta)^{-1}), \text{When } n \rightarrow \infty$$

Where $\mathcal{I}(\beta)$ is the Fisher information matrix:

$$\mathcal{I}(\beta) = \mathbb{E}\left[ -\frac{\partial^{2}l(\beta)}{\partial\beta\partial\beta^{T}} \right]$$
* The ”larger” (large eigenvalues) the matrix is, the more precise the estimation of $\beta$ is, because that results in smaller variances.

As seen before, it turns out that

$$\mathcal{I}(\beta) = \mathbf{X^{T}VX}$$
* $X$
* $\mathbf{V} = diag(V_{1}, ..., V_{n})$ with $V_{i} = \frac{1}{Var[Y_{i}]}(d\mu/d\eta)^{2}$
  * The $\mathbf{V}$ matrix is used as a weight matrix in the IRLS
  * Notice that in the gaussian linear regression (with identity link) $V_{i}$ are a constant for all i
