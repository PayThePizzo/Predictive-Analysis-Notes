# Inference

## Glm in R
The `glm` function fit a GLM model using the maximum likelihood method. For ordinary usage, the call is the same as for lm, except for one extra argument, family. The default value of family is gaussian, for each family we can specify a link function
* In the Poisson regression case the call looks like `glm(y~x, family = poisson)`
* In the logistic regression case, for example, the call looks like `glm(y~x, family = binomial)`

## glm Bernoulli example
```R
logitout <- glm(yesVote ~ . , data = chileElection, family = binomial)
```
The summary here prints out:
* Deviance Residuals, with the min, max and the quantiles
* Coefficients, the usual table with the estimations and significances.
  * Estimate 
  * Std. Error 
  * `z value` and `Pr(>|z|)`, which are test statistics on the single predictors. They are based on the distribution of the MLE, which is a gaussian distribution.
* A note `Dispersion parameter for binomial family taken to be 1` 
* Null deviance and the DoF, similar to R-Squared
* Residual deviance and the DoF, similar to the F-Statistic
* Number of observations deleted `54 observations deleted due to missingness`
* AIC
* Number of Fisher Scoring iterations

### Link
As a default R uses the canonical link, for the binomial we check `binomial()$link` which returns `"logit"`. The link can be changed - see ?family

### Variance 
Specifying the family implies the specification of the variance function:

```R
binomial()$variance

function (mu)
mu * (1 - mu)
<bytecode: 0x000001ee75be2310>
<environment: 0x000001ee764122d8>
```

### Names
Specifying the family and the link implies a number of relationships:

```R
names(binomial())

[1] "family" "link" "linkfun" "linkinv" "variance"
[6] "dev.resids" "aic" "mu.eta" "initialize" "validmu"
[11] "valideta" "simulate"
```

---

## Fitting issues for the logistic regression
We should note that, if there exists some $\beta^{*}$ such that $X\beta > 0 \Rightarrow y_{i}=1$ and $X\beta < 0 \Rightarrow y_{i}=0$, for all observations, then the MLE is not unique. Which is usually what happens when we have binary response variable.

![separabledata](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/Separabledataglm.png?raw=TRUE)

Such data is said to be separable. There are many ways to proceed with the estimations. 

This, and similar numeric issues related to estimated probabilities near 0 or 1, will return a warning in R.

```R
Warning messages:
1: glm.fit: algorithm did not converge
2: glm.fit: fitted probabilities numerically 0 or 1 occurred
```

When this happens, the model is still “fit”, but there are consequences, namely, the estimated coefficients are highly suspect.
* This is an issue when then trying to interpret the model.
* However it could be useful for creating a classifier

---

## Inference for model parameters

The assumptions on which a generalized linear model is constructed allow us to specify what is the asymptotic distribution of the random vector $\hat{\beta}$ through the theory of MLE. 
* The estimators are normally distributed, unbiased and with the lowest possible variance. 
* We assume that the randomness of Y comes only from $(Y |X_{1} = x_{1},..., X_{p−1} = x_{p−1})$ and not from the predictors.

We rewrite the relationship between the expected value of $\mu$ and the linear predictor in matrix form:

$$\eta = g(\mu) = X\beta$$
* $\beta$ is a vector of parameters that we wish to estimate: we estimate them using an algorithm that maximizes the likelihood
* so the beta parameters enjoy optimal (approximate) properties of MLEs

There is an important difference between the inference results for the Gaussian linear model
and for glm:
* In Gaussian linear model the inference is exact. 
  * This is due to the nice properties of the normal, least squares estimation, and linearity. As a consequence, the distributions of the coefficients are perfectly known assuming that the assumptions hold.
* In generalized linear models the inference is **asymptotic**. 
  * This means that the **distributions of the coefficients are unknown except for large sample sizes n**, for which we have approximations. 
  * Keep in mind that in GLM-like models the relationsip between $\mu(x)$ and $x$ is non-linear: we fit a more complex model and this hinders our ability to make inference.

In fact, we can show that:

$$\hat{\beta} \stackrel{approx}\thicksim  \mathcal{N}(\beta, \mathcal{I}(\beta)^{-1}), \text{ When } n \rightarrow \infty$$
* This is an asymptotical approximation
  

Where $\mathcal{I}(\beta)$ is the Fisher information matrix:

$$\mathcal{I}(\beta) = \mathbb{E}\left[ -\frac{\partial^{2}l(\beta)}{\partial\beta\partial\beta^{T}} \right]$$
* The ”larger” (large eigenvalues) the matrix is, the more precise the estimation of $\beta$ is, because that results in smaller variances.

As seen before, it turns out that

$$\mathcal{I}(\beta) = \mathbf{X^{T}VX}$$
* $X$, the design matrix
* $\mathbf{V} = diag(V_{1}, ..., V_{n})$ with $V_{i} = \frac{1}{Var[Y_{i}]}(d\mu/d\eta)^{2}$
  * The $\mathbf{V}$ matrix is used as a weight matrix in the IRLS
  * Notice that in the gaussian linear regression (with identity link) $V_{i}$ are a constant for all i
  
The uncertainty for the $\hat{\beta}$ parameters, also depends on the variance of the observed $Y$. $V_{i}$ values for noticeable distributions:

| Regression 	| $V_{i}$                                           	|
|------------	|---------------------------------------------------	|
| Logistic   	| $V_{i} = \frac{exp(X\beta)}{[1+exp(X\beta)]^{2}}$ 	|
| Poisson    	| $V_{i} = exp(X\beta)$                             	|

As we know, the estimation of the beta parameters depends on the variance, which in turn depends on the beta parameters. This can create quite an ugly situation, since we do not know the true beta parameters. 

The inverse of the Fisher information matrix is estimated by plugging in $\hat{\beta}$ into $\mathcal{I}(\beta)^{-1}$, namely $\mathcal{I}(\hat{\beta})^{-1}$.

### In conclusion
1. The estimates are **asymptotically** unbiased. The variance depends on:
   1. Sample size **n**, as n grows the precision of the estimators increases.
   2. Weighted predictor sparsity $(\mathbf{X}^{T}\mathbf{V}^{-1}\mathbf{X})^{-1}$, the more sparse the predictor is (small
eigenvalues of $(\mathbf{X}^{T}\mathbf{V}^{-1}\mathbf{X})^{-1}$), the more precise $\hat{\beta}$ is.
2. The **precision** of $\hat{\beta}$ is affected by the true value of $\beta$, which is “hidden” inside $\mathbf{V}$
   1. This is partially due to the heteroskedasticity of logistic regression and Poisson regression, which implies a dependence of the variance of $Y$ in the predictors, hence in $\beta$.
   2. This contrasts sharply with the linear model, where the precision of the least squares estimator was not affected by the value of the unknown coefficients.

---

## Confidence intervals for the coefficients
Similar to linear regression, the problem is that $\mathbf{V}$ is unknown in practice because it depends on $\beta$. Plugging in the estimates $\hat{\beta}$ results on $\mathbf{\hat{V}}$. We can use $\mathbf{\hat{V}}$ to get:

$$Z_{j} = \frac{\hat{\beta_{j}}-\beta_{j}}{SE(\beta_{j})} \stackrel{approx}\thicksim \mathcal{N}(0,1)$$
* where $SE(\hat{\beta_{j}})^{2} = v_{j}$ with $v_{j}$ is the j-th element of the diagonal of  $(\mathbf{X}^{T}\mathbf{\hat{V}}^{-1}\mathbf{X})^{-1}$ 

Thanks to normal approximation, we can have the $100(1 − \alpha)$% confidence intervals for the coefficient $\beta_{j}$

$$\hat{\beta_{j}} \pm z_{1-\alpha/2}SE(\hat{\beta_{j}})$$
* where $z_{1-\alpha/2}$ is the $1 − \alpha/2$-upper quantile of the $\mathcal{N}(0, 1)$.

## Testing with GLMs: Wald test
System of hypothesis

$$H_{0} : \beta_{j} = 0 \text{   vs   }H_{A} :\beta_{j} \neq 0$$

The distribution of the test statistic is no longer T since the t-test for ordinary linear regression, assuming the assumptions were correct, had an exact distribution for any sample size. Here instead, the result are approximated.

$$W = \frac{\hat{\beta_{j}}}{SE[\hat{\beta_{j}}]}$$

But for a large number of observations $n \rightarrow \infty$:

$$W \stackrel{approx}\thicksim \mathcal{N}(0,1)$$

R will obtain the standard error for us. The use of this test will be extremely similar to the t-test for ordinary linear regression. Essentially the only thing that changes is the distribution of the test statistic.

What stays the same are those criteria which are based on the likelihood
* Nested models evaluation, Information Criteria (AIC, BIC, ...).

Other measures like the ANOVA table and the R-squared cannot be used, so they must be replaced. This is what we are going to do now.

---

## GLM: the Poisson Model
This problem refers to data from a study of nesting horseshoe crabs (J. Brockmann, Ethology 1996)
* Available from the GLMsData package

Each female horseshoe crab in the study had a male crab attached to her in her nest. The study investigated factors that affect whether
the female crab had any other males, called satellites, residing near her.

The response outcome for each female crab is her number of satellites (Sa). The eplanatory variables that are thought to affect this included
* The female crab’s color (C),
* Spine condition (S), 
* Weight (Wt)
* Carapace width (W). 

As first model we can use a "baseline model" which implies $\mathbb{E}[Y|X_{i} = x_{i}] = exp(\beta_{0})$
* If $Y \thicksim Pois(\mu)$, then we could suppose that a good estimator is $\hat{\mu} = \bar{x}$

```R
model0<-glm(Sat~1, family=poisson(link=log),data=hcrabs)
```

Then we fit a model including the carapace width and weight as predictors $\mathbb{E}[Y|X_{i} = x_{i}] = exp(\beta_{0} + \beta_{1}W + \beta_{2}Wt)$

```R
model1<-glm(Sat~1+Width+Wt,family=poisson(link=log), data=hcrabs)
```

Natural question: does adding the predictors ”improve” the model?
* Notice that we have fitted two nested models!

$$H_{0} : \beta_{1} = \beta_{2} = 0 \text{   vs   }H_{A} :\beta_{1} \neq \text{  or  } \beta_{2} \neq 0$$


## Testing with GLMs: likelihood-ratio test
To generalize this situation let's think of:
* A full model $g(\mathbb{E}[Y|X_{1} = x_{1},..., X_{p-1}=x_{p-1}]) = \beta_{0}+ \beta_{1}x_{1},..., \beta_{p-1}x_{p-1}$
  * The MLE of these beta parameters is denoted $\hat{\beta}_{full}$
* A null model $g(\mathbb{E}[Y|X_{1} = x_{1},..., X_{p-1}=x_{p-1}]) = \beta_{0}+ \beta_{1}x_{1},..., \beta_{p-1}x_{p-1}$
  * With $q < p$
  * The MLE of these beta parameters is denoted $\hat{\beta}_{null}$

The difference between these two models can be codified by the null hypothesis of a test:

$$H_{0}: \beta_{q} =  \beta_{q+1} = ... = \beta_{p-1} = 0$$

We define a test statistic LR "Likelihood Ratio"

$$LR = -2\log\left(\frac{L(\hat{\beta}_{null})}{L(\hat{\beta}_{full})}\right)= 2\log\frac{L(\hat{\beta}_{full})}{L(\hat{\beta}_{null})} = 2(l(\hat{\beta}_{full})-l(\hat{\beta}_{null}))$$

For a large enough sample, this test statistic has an approximate Chi-square distribution

$$LR \stackrel{approx}\thicksim \chi^{2}_{p-q}$$

The test, which we will call the **Likelihood-Ratio Test** (LRT), will be the analogue to the ANOVA F-test for linear regression.
* To perform the LRT, we’ll actually again use the `anova` function in R
* The LRT is a rather general test, however, here we have presented a specific application to GLMs.

As the R-squared, this statistics is useful but can lead to overfitting as new predictors are added to the model.

### In R
We use the LRT test to compare the two models for the horseshoe crabs
```R
logLik(model0); logLik(model1)
'log Lik.' -494.0447 (df=1)
'log Lik.' -457.5991 (df=3)

(tstat <- as.numeric(2*(logLik(model1) - logLik(model0))))
[1] 72.89106

diff_df <- length(model1$coefficients) - length(model0$coefficients)

# pvalue
pchisq(tstat, df = diff_df, lower.tail = FALSE)
[1] 1.485621e-16
```

Or we can use directly the anova function in R:
* Notice that we need to specify the test we wish to perform
```R
anova(model0, model1, test = "LRT")

Analysis of Deviance Table
Model 1: Sat ˜ 1
Model 2: Sat ˜ 1 + Width + Wt

  Resid. Df   Resid. Dev  Df  Deviance  Pr(>Chi)
1   172       632.79
2   170       559.90      2   72.891    < 2.2e-16 ***
---
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
The table produce by anova is referred to as the analysis of **deviance table**. The deviance plays in GLM the same role played by the sum of squares in gaussian linear regression.

---

## Deviance
The deviance for a model is defined as:

$$D = 2[l(\hat{\beta}_{max}) - l(\hat{\beta})]\phi = \frac{\phi}{a(\phi)}[y(\hat{\theta}^{sat} - \hat{\theta})-b(\hat{\theta}^{sat})+b(\hat{\theta})]$$
* where $l(\hat{\beta}_{max})$  is the maximized likelihood under the saturated model and $\hat{\theta}^{sat}$ is the estimated value of $\theta$ in the saturated model

It is two times the difference between the largest log-likelihood we can find and the log-likelihood of the estimated model, multiplied by $\phi$.
* Mind that $\phi$ complicates the computations since for each estimated model, we have different values of $\phi$. Furthermore, due to the link between the expected value and the variance, deviances become hard to find when $\phi \neq 1$.
  * The families of functions we are focusing on, have $\phi=1$ so we can avoid this issue for now.

### Saturated model
What is the saturated model? The model
1. In which we have a **n different estimations** for $\theta_{i}$, one for each observation. 
2. Which has the highest possible likelihood value for the data.
3. Which has n-parameters and n-observations.
4. Where the deviance (sum of squares residuals) is 0.

![satmod](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/satmod.png?raw=TRUE)

The best estimation for the expected value of an observation, under the assumption that each observation has a different estimation, would be that the estimated value of $Y_{i}$ is exactly $Y_{i}$ itself. 

### Residual deviance
From above, we can derive the residual deviance

![resdev](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/resdev.png?raw=TRUE)

We need to remember that: 

$$l^{sat} = l(\hat{\theta}_{i}), \text{ where } \hat{\theta}_{i} = b'^{-1}(y_{i})$$

Notice that $d_{i}$ is the individual deviance, namely the contribute that each single observation gives to the deviance. This will be denoted later as the residuals of the deviance. In the ordinary linear model $D$ corresponds to the $SS_{RES}$.

In fact, the deviance has this precise role: it indicates how far our estimate is from the model where each observation is estimated with the $y_{i}$. 

It is important to highlight that we can use this form since we are use MLE estimators and always revolving around this concept for our estimations.

#### Poisson regression
When considering a Poisson regression we need to remember:
* $Y \thicksim Pois(\mu) \rightarrow \mu^{y}exp^{-\mu}$

![poisdev1](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/poisdev.png?raw=TRUE)

![poisdev2](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/poisdev2.png?raw=TRUE)

The deviance is still a number, and needs to be analyzed in order to be used as a goodness-of-fit measure.

### Null deviance
The null deviance is the deviance for the models with only one parameter, for example the null model where $\hat{\mu}^{null} = exp(\beta_{0})$. 
* The deviance for the null model is the largest possible deviance.

Null deviance is 2 times the log of the ratio between the likelihood $L^{sat}$ for the saturated model and the likelihood $L(\hat{\beta}_{0})$ for the fitted model with only intercept (which is the simplest possible model).




In R we can access the deviance in the summary of trough the deviance function:

```R
summary(model1)$deviance; summary(model1)$null.dev
[1] 559.9006
[1] 632.7917

deviance(model1); deviance(model0)
[1] 559.9006
[1] 632.7917
```

### Scale deviance
We can also defined the scale deviance which does not depend on $\phi$:

$$D^{*} = D/\phi$$

and, in the case in which $\phi$ in known, the LRT can be rewritten as the difference in deviances between two models

$$D(\mathcal{M}_{1}) − D(\mathcal{M}_{2}) = 2[I(\hat{\beta}^{\mathcal{M}_{2}}) - I(\hat{\beta}^{\mathcal{M}_{1}})]$$

This is no longer true when $\phi$ needs to be estimated (for example, in a gaussian or gamma distribution)

## Goodness of fit
LRT only holds for nested models, how can we compare non-nested models?

We need general purpose measures of goodness of fit. We use AIC (and BIC) with the same interpretation as for linear models:

```R
AIC(model1)
[1] 921.1983

2*(-as.numeric(logLik(model1)) + length(model1$coefficients))
[1] 921.1983
```

---

# Logistic regression use - SAheart example
To illustrate the use of logistic regression, we will use the SAheart dataset from the book The Elements of Statistical Learning.

```R
fl <- "http://www-stat.stanford.edu/˜tibs/ElemStatLearn/datasets/SAheart"
SAheart <- read.table(fl, sep=",",head=T,row.names=1)

  sbp tobacco ldl   adiposity   famhist   typea obesity alcohol   age chd
1 160 12.00   5.73  23.11       Present   49    25.30     97.20   52  1
2 144 0.01    4.41  28.61       Absent    55    28.87     2.06    63  1
3 118 0.08    3.48  32.28       Present   52    29.14     3.81    46  0
4 170 7.50    6.41  38.03       Present   51    31.99     24.26   58  1
5 134 13.60   3.50  27.78       Present   60    25.99     57.34   49  1
```

This data comes from a retrospective sample of males in a heart-disease high-risk region of the Western Cape, South Africa.
* The response variable is chd, which indicates whether or not coronary heart disease is present in an individual. (numeric 0 / 1 variable.
* Predictors: various measurements for each individual, many related to heart health. For example sbp, systolic blood pressure, and ldl, low density lipoprotein cholesterol.

First we model the probability of coronary heart disease based on low density lipoprotein cholesterol:

$$log \left( \frac{Pr[chd=1]}{1-Pr[chd=1]} \right) = \beta_{0} + \beta_{1dl}x_{1dl}$$

![probchd](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/probchd.png?raw=TRUE)

### Hypothesis testing and confidence interval
As we would expect as ldl increases, so does the probability of chd. We wish to carry out a test for $H_{0}: \beta_{1dl} = 0$ vs $H_{A}: \beta_{1dl} \neq 0$

We find the test statistic and p-value for the test using the summary function
```R
coef(summary(chd_mod_ldl))
              Estimate    Std. Error  z value     Pr(>|z|)
(Intercept)   -1.9686681  0.27307908  -7.209150   5.630207e-13
ldl           0.2746613   0.05163983  5.318787    1.044615e-07
```

We have a low p-value and reject H0: ldl appears to be a relevant predictor.

Confidence intervals at 95%
```R
confint.default(chd_mod_ldl,level=0.95)
2.5 % 97.5 %
(Intercept) -2.5038933 -1.4334430
ldl 0.1734491 0.3758735
```

To fit an additive model using all available predictors, we use:

```R
chd_mod_additive <- glm(chd ˜ ., data = SAheart, family = binomial)
```

We use the likelihood-ratio test to compare the two model. Specifically, we are testing
H0 : βsbp = βtobacco = βadiposity = βfamhist = βtypea = βobesity = βalcohol = βage = 0

The LR test statistic,
```R
-2 * as.numeric(logLik(chd_mod_ldl) - logLik(chd_mod_additive))
[1] 92.13879
```

We can also use the anova function, by specifying test = "LRT"

```R
anova(chd_mod_ldl, chd_mod_additive, test = "LRT")
Analysis of Deviance Table
Model 1: chd ˜ ldl
Model 2: chd ˜ sbp + tobacco + ldl + adiposity + famhist + typea + obesity +
alcohol + age
Resid. Df Resid. Dev Df Deviance Pr(>Chi)
1 460 564.28
2 452 472.14 8 92.139 < 2.2e-16 ***
---
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

The small p-value suggests that we prefer the larger model.
• To select a subset of predictors, we can use a stepwise procedure as we did with ordinary
linear regression.
• Recall that AIC and BIC were defined in terms of likelihoods.

• Example using AIC with a backwards selection procedure.

```R
chd_mod_selected <- step(chd_mod_additive, trace = 1, k = 2)
Start: AIC=492.14
chd ˜ sbp + tobacco + ldl + adiposity + famhist + typea + obesity +
alcohol + age
Df Deviance AIC
- alcohol 1 472.14 490.14
- adiposity 1 472.55 490.55
- sbp 1 473.44 491.44
<none> 472.14 492.14
- obesity 1 474.23 492.23
- ldl 1 481.07 499.07
- tobacco 1 481.67 499.67
- typea 1 483.05 501.05
- age 1 486.53 504.53
- famhist 1 488.89 506.89
Step: AIC=490.14

chd ˜ sbp + tobacco + ldl + adiposity + famhist + typea + obesity +
age
Df Deviance AIC
- adiposity 1 472.55 488.55
- sbp 1 473.47 489.47
<none> 472.14 490.14
- obesity 1 474.24 490.24
- ldl 1 481.15 497.15
- tobacco 1 482.06 498.06
- typea 1 483.06 499.06
- age 1 486.64 502.64
- famhist 1 488.99 504.99
Step: AIC=488.55
chd ˜ sbp + tobacco + ldl + famhist + typea + obesity + age
Df Deviance AIC
- sbp 1 473.98 487.98

<none> 472.55 488.55
- obesity 1 474.65 488.65
- tobacco 1 482.54 496.54
- ldl 1 482.95 496.95
- typea 1 483.19 497.19
- famhist 1 489.38 503.38
- age 1 495.48 509.48
Step: AIC=487.98
chd ˜ tobacco + ldl + famhist + typea + obesity + age
Df Deviance AIC
- obesity 1 475.69 487.69
<none> 473.98 487.98
- tobacco 1 484.18 496.18
- typea 1 484.30 496.30
- ldl 1 484.53 496.53
- famhist 1 490.58 502.58
- age 1 502.11 514.11

Step: AIC=487.69
chd ˜ tobacco + ldl + famhist + typea + age
Df Deviance AIC
<none> 475.69 487.69
- ldl 1 484.71 494.71
- typea 1 485.44 495.44
- tobacco 1 486.03 496.03
- famhist 1 492.09 502.09
- age 1 502.38 512.38

coef(chd_mod_selected)
(Intercept) tobacco ldl famhistPresent typea
-6.44644451 0.08037533 0.16199164 0.90817526 0.03711521
age
0.05046038
```

We could again compare this model to the additive models.
H0 : βsbp = βadiposity = βobesity = βalcohol = 0

```R
anova(chd_mod_selected, chd_mod_additive, test = "LRT")
Analysis of Deviance Table
Model 1: chd ˜ tobacco + ldl + famhist + typea + age
Model 2: chd ˜ sbp + tobacco + ldl + adiposity + famhist + typea + obesity +
alcohol + age
Resid. Df Resid. Dev Df Deviance Pr(>Chi)
1 456 475.69
2 452 472.14 4 3.5455 0.471

```
• Here it seems that we would prefer the selected model with 5 predictors.


---

## Prediction

![predglm](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/predglm.png?raw=TRUE)

In R we can obtain a prediction for both ˆη and ˆµ = g−1(ˆη):

```R
## back to the hcrabs example
head(predict(model1, type = "link"),4)
1 2 3 4
1.3720122 0.4343135 0.9308087 0.7861229
exp(head(predict(model1, type = "link"),4))
1 2 3 4
3.943277 1.543903 2.536560 2.194870
head(predict(model1, type = "response"),4)
1 2 3 4
3.943277 1.543903 2.536560 2.194870
```

![predglm2](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/predglm2.png?raw=TRUE)


```R
lpred <- predict(model1, type = "link", se.fit=TRUE)
cbind(lpred$fit[1:4] + qnorm(.025)*lpred$se.fit[1:4],
lpred$fit[1:4] + qnorm(.975)*lpred$se.fit[1:4])
[,1] [,2]
1 1.2716445 1.4723799
2 0.2372926 0.6313344
3 0.8315315 1.0300860
4 0.6628823 0.9093635
exp(cbind(lpred$fit[1:4] + qnorm(.025)*lpred$se.fit[1:4],
lpred$fit[1:4] + qnorm(.975)*lpred$se.fit[1:4]))
[,1] [,2]
1 3.566713 4.359598
2 1.267812 1.880118
3 2.296834 2.801307
4 1.940377 2.482742

```

---

## GLM Residuals

• Residuals represent the difference between the data and the model and are essential to
explore the adequacy of the model.
• In the Gaussian case, the residuals are $r_{i} = y_{i} − \hat{y}_{i}$
.
• These are called response residuals for GLMs, but since the variance of the response is
not constant for most GLMs, some modification is necessary.
• The Pearson residuals are comparable to the standardized residuals used in linear models
and is defined as:

$$r^{p}_{i} = \frac{y_{i}-\hat{y}_{i}}{\sqrt{\hat{V}_{i}}}$$

![glmres](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/glmres.png?raw=TRUE)

(Crab data) We can obtain the deviance residuals as:
```R
residuals(model1)[1:5]
1 2 3 4 5
1.7903699 -1.7572153 3.1414336 -2.0951707 0.6101256
```
These are the default choice of residuals.
• The Pearson residuals are:
```R
residuals(model1,"pearson")[1:5]
1 2 3 4 5
2.0428979 -1.2425388 4.0582724 -1.4815094 0.6455594
```
If we use
```R
model1$residuals[1:5]
1 2 3 4 5
1.0287693 -1.0000000 2.5481128 -1.0000000 0.3790497
```

we obtain the working residuals, i.e.

$$\hat{\eta}_{i} + (y_{i} - \hat{\mu}_{i})\frac{d\eta_{i}}{d\mu_{i}}$$

For linear models, the plot of residuals against fitted values is probably the single most valuable graphic for model diagnostics.
• For GLMs: which residuals? Which scale (linear predictor or original scale?)
• In GLMs residual plots are much harder to read and verifying assumptions is more
complicated (so we don’t really focus on it during the course)

For GLMs, we must decide on the appropriate scale for the fitted values. Usually, it is
better to plot the fitted linear predictors.

$$\hat{\beta}_{0} + \hat{\beta}_{1}x_{i,1} + ... + \hat{\beta}_{p-1}x_{i,p-1}$$

rather than the predicted responses ˆyi
.
• Scatter plot of Deviance residuals against xj used to check whether any systematic
relationship is present: if so include xj
in the model.
• Plot working residuals against linear predictor. Plot should be linear - if not the link
function might be not correctly specified.
• Influential and leverage points can still be problematic: we can define leverage-aware
residuals, but we don’t discuss them in the course

### Example Galapagos Data

For 30 Gal´apagos Islands, we have a count of the number of plant species found on each
island and the number that are endemic to that island. We also have five geographic
variables for each island.
• We model the number of species using normal linear regression:
```R
data(gala, package="faraway")
gala <- gala[,-2]
```
• We throw out the Endemics variable (which falls in the second column of the dataframe)
since we won’t be using it in this analysis. We fit a linear regression and look at the
residual vs. fitted plot:

```R
modl <- lm(Species ˜ . , gala)
plot(modl, 1)
```

![galapagosres](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/galapagosres.png?raw=TRUE)

We see clear evidence of nonconstant variance
• the Box-Cox method reveals that a square-root transformation is a sensible transformation

```R
library(MASS)
boxcox(modl, plotit = TRUE)
```

![galapagosboxcox](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/galapagosboxcox.png?raw=TRUE)

```R
modt <- lm(sqrt(Species) ˜ . , gala)
summary(modt)

Call:
lm(formula = sqrt(Species) ˜ ., data = gala)
Residuals:
Min 1Q Median 3Q Max
-4.5572 -1.4969 -0.3031 1.3527 5.2110
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) 3.3919243 0.8712678 3.893 0.000690 ***
Area -0.0019718 0.0010199 -1.933 0.065080 .
Elevation 0.0164784 0.0024410 6.751 5.55e-07 ***
Nearest 0.0249326 0.0479495 0.520 0.607844
Scruz -0.0134826 0.0097980 -1.376 0.181509
Adjacent -0.0033669 0.0008051 -4.182 0.000333 ***
---
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 2.774 on 24 degrees of freedom
Multiple R-squared: 0.7827, Adjusted R-squared: 0.7374
F-statistic: 17.29 on 5 and 24 DF, p-value: 2.874e-07
```

```R
plot(modt, 1)
```

![galapagosres2](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/galapagosres2.png?raw=TRUE)

 We achieved this fit at the cost of transforming the response. This makes interpretation
more difficult.
• Furthermore, some of the response values are quite small (single digits) which makes us
question the validity of the normal approximation.
• This model may be adequate, but perhaps we can do better.