# Inference

## Glm in R
The `glm` function fit a GLM model using the maximum likelihood method. For ordinary usage, the call is the same as for lm, except for one extra argument, family. The default value of family is gaussian, for each family we can specify a link function
* In the Poisson regression case the call looks like `glm(y~x, family = poisson)`
* In the logistic regression case, for example, the call looks like `glm(y~x, family = binomial)`

## glm Bernoulli example
```R
logitout <- glm(yesVote ~ . , data = chileElection, family = binomial)
```
The summary here prints out:
* Deviance Residuals, with the min, max and the quantiles
* Coefficients, the usual table with the estimations and significances.
  * Estimate 
  * Std. Error 
  * `z value` and `Pr(>|z|)`, which are test statistics on the single predictors. They are based on the distribution of the MLE, which is a gaussian distribution.
* A note `Dispersion parameter for binomial family taken to be 1` 
* Null deviance and the DoF, similar to R-Squared
* Residual deviance and the DoF, similar to the F-Statistic
* Number of observations deleted `54 observations deleted due to missingness`
* AIC
* Number of Fisher Scoring iterations

### Link
As a default R uses the canonical link, for the binomial we check `binomial()$link` which returns `"logit"`. The link can be changed - see ?family

### Variance 
Specifying the family implies the specification of the variance function:

```R
binomial()$variance

function (mu)
mu * (1 - mu)
<bytecode: 0x000001ee75be2310>
<environment: 0x000001ee764122d8>
```

### Names
Specifying the family and the link implies a number of relationships:

```R
names(binomial())

[1] "family" "link" "linkfun" "linkinv" "variance"
[6] "dev.resids" "aic" "mu.eta" "initialize" "validmu"
[11] "valideta" "simulate"
```

---

## Fitting issues for the logistic regression
We should note that, if there exists some $\beta^{*}$ such that $X\beta > 0 \Rightarrow y_{i}=1$ and $X\beta < 0 \Rightarrow y_{i}=0$, for all observations, then the MLE is not unique. Which is usually what happens when we have binary response variable.

![separabledata](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/Separabledataglm.png?raw=TRUE)

Such data is said to be separable. There are many ways to proceed with the estimations. 

This, and similar numeric issues related to estimated probabilities near 0 or 1, will return a warning in R.

```R
Warning messages:
1: glm.fit: algorithm did not converge
2: glm.fit: fitted probabilities numerically 0 or 1 occurred
```

When this happens, the model is still “fit”, but there are consequences, namely, the estimated coefficients are highly suspect.
* This is an issue when then trying to interpret the model.
* However it could be useful for creating a classifier

---

## Inference for model parameters

The assumptions on which a generalized linear model is constructed allow us to specify what is the asymptotic distribution of the random vector $\hat{\beta}$ through the theory of MLE. 
* The estimators are normally distributed, unbiased and with the lowest possible variance. 
* We assume that the randomness of Y comes only from $(Y |X_{1} = x_{1},..., X_{p−1} = x_{p−1})$ and not from the predictors.

We rewrite the relationship between the expected value of $\mu$ and the linear predictor in matrix form:

$$\eta = g(\mu) = X\beta$$
* $\beta$ is a vector of parameters that we wish to estimate: we estimate them using an algorithm that maximizes the likelihood
* so the beta parameters enjoy optimal (approximate) properties of MLEs

There is an important difference between the inference results for the Gaussian linear model
and for glm:
* In Gaussian linear model the inference is exact. 
  * This is due to the nice properties of the normal, least squares estimation, and linearity. As a consequence, the distributions of the coefficients are perfectly known assuming that the assumptions hold.
* In generalized linear models the inference is **asymptotic**. 
  * This means that the **distributions of the coefficients are unknown except for large sample sizes n**, for which we have approximations. 
  * Keep in mind that in GLM-like models the relationsip between $\mu(x)$ and $x$ is non-linear: we fit a more complex model and this hinders our ability to make inference.

In fact, we can show that:

$$\hat{\beta} \stackrel{approx}\thicksim  \mathcal{N}(\beta, \mathcal{I}(\beta)^{-1}), \text{ When } n \rightarrow \infty$$
* This is an asymptotical approximation
  

Where $\mathcal{I}(\beta)$ is the Fisher information matrix:

$$\mathcal{I}(\beta) = \mathbb{E}\left[ -\frac{\partial^{2}l(\beta)}{\partial\beta\partial\beta^{T}} \right]$$
* The ”larger” (large eigenvalues) the matrix is, the more precise the estimation of $\beta$ is, because that results in smaller variances.

As seen before, it turns out that

$$\mathcal{I}(\beta) = \mathbf{X^{T}VX}$$
* $X$, the design matrix
* $\mathbf{V} = diag(V_{1}, ..., V_{n})$ with $V_{i} = \frac{1}{Var[Y_{i}]}(d\mu/d\eta)^{2}$
  * The $\mathbf{V}$ matrix is used as a weight matrix in the IRLS
  * Notice that in the gaussian linear regression (with identity link) $V_{i}$ are a constant for all i
  
The uncertainty for the $\hat{\beta}$ parameters, also depends on the variance of the observed $Y$. $V_{i}$ values for noticeable distributions:

| Regression 	| $V_{i}$                                           	|
|------------	|---------------------------------------------------	|
| Logistic   	| $V_{i} = \frac{exp(X\beta)}{[1+exp(X\beta)]^{2}}$ 	|
| Poisson    	| $V_{i} = exp(X\beta)$                             	|

As we know, the estimation of the beta parameters depends on the variance, which in turn depends on the beta parameters. This can create quite an ugly situation, since we do not know the true beta parameters. 

The inverse of the Fisher information matrix is estimated by plugging in $\hat{\beta}$ into $\mathcal{I}(\beta)^{-1}$, namely $\mathcal{I}(\hat{\beta})^{-1}$.

### In conclusion
1. The estimates are **asymptotically** unbiased. The variance depends on:
   1. Sample size **n**, as n grows the precision of the estimators increases.
   2. Weighted predictor sparsity $(\mathbf{X}^{T}\mathbf{V}^{-1}\mathbf{X})^{-1}$, the more sparse the predictor is (small
eigenvalues of $(\mathbf{X}^{T}\mathbf{V}^{-1}\mathbf{X})^{-1}$), the more precise $\hat{\beta}$ is.
2. The **precision** of $\hat{\beta}$ is affected by the true value of $\beta$, which is “hidden” inside $\mathbf{V}$
   1. This is partially due to the heteroskedasticity of logistic regression and Poisson regression, which implies a dependence of the variance of $Y$ in the predictors, hence in $\beta$.
   2. This contrasts sharply with the linear model, where the precision of the least squares estimator was not affected by the value of the unknown coefficients.

---

## Confidence intervals for the coefficients
Similar to linear regression, the problem is that $\mathbf{V}$ is unknown in practice because it depends on $\beta$. Plugging in the estimates $\hat{\beta}$ results on $\mathbf{\hat{V}}$. We can use $\mathbf{\hat{V}}$ to get:

$$Z_{j} = \frac{\hat{\beta_{j}}-\beta_{j}}{SE(\beta_{j})} \stackrel{approx}\thicksim \mathcal{N}(0,1)$$
* where $SE(\hat{\beta_{j}})^{2} = v_{j}$ with $v_{j}$ is the j-th element of the diagonal of  $(\mathbf{X}^{T}\mathbf{\hat{V}}^{-1}\mathbf{X})^{-1}$ 

Thanks to normal approximation, we can have the $100(1 − \alpha)$% confidence intervals for the coefficient $\beta_{j}$

$$\hat{\beta_{j}} \pm z_{1-\alpha/2}SE(\hat{\beta_{j}})$$
* where $z_{1-\alpha/2}$ is the $1 − \alpha/2$-upper quantile of the $\mathcal{N}(0, 1)$.

## Testing with GLMs: Wald test
System of hypothesis

$$H_{0} : \beta_{j} = 0 \text{   vs   }H_{A} :\beta_{j} \neq 0$$

The distribution of the test statistic is no longer T since the t-test for ordinary linear regression, assuming the assumptions were correct, had an exact distribution for any sample size. Here instead, the result are approximated.

$$W = \frac{\hat{\beta_{j}}}{SE[\hat{\beta_{j}}]}$$

But for a large number of observations $n \rightarrow \infty$:

$$W \stackrel{approx}\thicksim \mathcal{N}(0,1)$$

R will obtain the standard error for us. The use of this test will be extremely similar to the t-test for ordinary linear regression. Essentially the only thing that changes is the distribution of the test statistic.

What stays the same are those criteria which are based on the likelihood
* Nested models evaluation, Information Criteria (AIC, BIC, ...).

Other measures like the ANOVA table and the R-squared cannot be used, so they must be replaced. This is what we are going to do now.

---

## GLM: the Poisson Model
This problem refers to data from a study of nesting horseshoe crabs (J. Brockmann, Ethology 1996)
* Available from the GLMsData package

Each female horseshoe crab in the study had a male crab attached to her in her nest. The study investigated factors that affect whether
the female crab had any other males, called satellites, residing near her.

The response outcome for each female crab is her number of satellites (Sa). The eplanatory variables that are thought to affect this included
* The female crab’s color (C),
* Spine condition (S), 
* Weight (Wt)
* Carapace width (W). 

As first model we can use a "baseline model" which implies $\mathbb{E}[Y|X_{i} = x_{i}] = exp(\beta_{0})$
* If $Y \thicksim Pois(\mu)$, then we could suppose that a good estimator is $\hat{\mu} = \bar{x}$

```R
model0<-glm(Sat~1, family=poisson(link=log),data=hcrabs)
```

Then we fit a model including the carapace width and weight as predictors $\mathbb{E}[Y|X_{i} = x_{i}] = exp(\beta_{0} + \beta_{1}W + \beta_{2}Wt)$

```R
model1<-glm(Sat~1+Width+Wt,family=poisson(link=log), data=hcrabs)
```

Natural question: does adding the predictors ”improve” the model?
* Notice that we have fitted two nested models!

$$H_{0} : \beta_{1} = \beta_{2} = 0 \text{   vs   }H_{A} :\beta_{1} \neq \text{  or  } \beta_{2} \neq 0$$


## Testing with GLMs: likelihood-ratio test
To generalize this situation let's think of:
* A full model $g(\mathbb{E}[Y|X_{1} = x_{1},..., X_{p-1}=x_{p-1}]) = \beta_{0}+ \beta_{1}x_{1},..., \beta_{p-1}x_{p-1}$
  * The MLE of these beta parameters is denoted $\hat{\beta}_{full}$
* A null model $g(\mathbb{E}[Y|X_{1} = x_{1},..., X_{p-1}=x_{p-1}]) = \beta_{0}+ \beta_{1}x_{1},..., \beta_{p-1}x_{p-1}$
  * With $q < p$
  * The MLE of these beta parameters is denoted $\hat{\beta}_{null}$

The difference between these two models can be codified by the null hypothesis of a test:

$$H_{0}: \beta_{q} =  \beta_{q+1} = ... = \beta_{p-1} = 0$$

We define a test statistic LR "Likelihood Ratio"

$$LR = -2\log\left(\frac{L(\hat{\beta}_{null})}{L(\hat{\beta}_{full})}\right)= 2\log\frac{L(\hat{\beta}_{full})}{L(\hat{\beta}_{null})} = 2(l(\hat{\beta}_{full})-l(\hat{\beta}_{null}))$$

For a large enough sample, this test statistic has an approximate Chi-square distribution

$$LR \stackrel{approx}\thicksim \chi^{2}_{p-q}$$

The test, which we will call the **Likelihood-Ratio Test** (LRT), will be the analogue to the ANOVA F-test for linear regression.
* To perform the LRT, we’ll actually again use the `anova` function in R
* The LRT is a rather general test, however, here we have presented a specific application to GLMs.

As the R-squared, this statistics is useful but can lead to overfitting as new predictors are added to the model.

### In R
We use the LRT test to compare the two models for the horseshoe crabs
```R
logLik(model0); logLik(model1)
'log Lik.' -494.0447 (df=1)
'log Lik.' -457.5991 (df=3)

(tstat <- as.numeric(2*(logLik(model1) - logLik(model0))))
[1] 72.89106

diff_df <- length(model1$coefficients) - length(model0$coefficients)

# pvalue
pchisq(tstat, df = diff_df, lower.tail = FALSE)
[1] 1.485621e-16
```

Or we can use directly the anova function in R:
* Notice that we need to specify the test we wish to perform
```R
anova(model0, model1, test = "LRT")

Analysis of Deviance Table
Model 1: Sat ˜ 1
Model 2: Sat ˜ 1 + Width + Wt

  Resid. Df   Resid. Dev  Df  Deviance  Pr(>Chi)
1   172       632.79
2   170       559.90      2   72.891    < 2.2e-16 ***
---
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
The table produce by anova is referred to as the analysis of **deviance table**. The deviance plays in GLM the same role played by the sum of squares in gaussian linear regression.

---

## Deviance
The deviance for a model is defined as:

$$D = 2[l(\hat{\beta}_{max}) - l(\hat{\beta})]\phi = \frac{\phi}{a(\phi)}[y(\hat{\theta}^{sat} - \hat{\theta})-b(\hat{\theta}^{sat})+b(\hat{\theta})]$$
* where $l(\hat{\beta}_{max})$  is the maximized likelihood under the saturated model and $\hat{\theta}^{sat}$ is the estimated value of $\theta$ in the saturated model

It is two times the difference between the largest log-likelihood we can find and the log-likelihood of the estimated model, multiplied by $\phi$.
* Mind that $\phi$ complicates the computations since for each estimated model, we have different values of $\phi$. Furthermore, due to the link between the expected value and the variance, deviances become hard to find when $\phi \neq 1$.
  * The families of functions we are focusing on, have $\phi=1$ so we can avoid this issue for now.

### Saturated model
What is the saturated model? The model
1. In which we have a **n different estimations** for $\theta_{i}$, one for each observation. 
2. Which has the highest possible likelihood value for the data.
3. Which has n-parameters and n-observations.
4. Where the sum of squares residuals is 0.

![satmod](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/satmod.png?raw=TRUE)

The best estimation for the expected value of an observation, under the assumption that each observation has a different estimation, would be that the estimated value of $Y_{i}$ is exactly $Y_{i}$ itself. 

### Residual deviance
From above, we can derive the residual deviance

![resdev](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/resdev.png?raw=TRUE)

We need to remember that: 

$$l^{sat} = l(\hat{\theta}_{i}), \text{ where } \hat{\theta}_{i} = b'^{-1}(y_{i})$$

Notice that $d_{i}$ is the individual deviance, namely the contribute that each single observation gives to the deviance. This will be denoted later as the residuals of the deviance. In the ordinary linear model $D$ corresponds to the $SS_{RES}$.

In fact, the deviance has this precise role: it indicates how far our estimate is from the model where each observation is estimated with the $y_{i}$. 

It is important to highlight that we can use this form since we are use MLE estimators and always revolving around this concept for our estimations.

#### Poisson regression
When considering a Poisson regression we need to remember:
* $Y \thicksim Pois(\mu) \rightarrow \mu^{y}exp^{-\mu}$

![poisdev1](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/poisdev.png?raw=TRUE)

![poisdev2](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/poisdev2.png?raw=TRUE)

The deviance is still a number, and needs to be analyzed in order to be used as a goodness-of-fit measure.

### Null deviance
Null deviance is 2 times the log of the ratio between the likelihood $L^{sat}$ for the saturated model and the likelihood $L(\hat{\beta}_{0})$ for the fitted model with only intercept (which is the simplest possible model).

In R we can access the deviance in the summary of trough the deviance function:

```R
summary(model1)$deviance; summary(model1)$null.dev
[1] 559.9006
[1] 632.7917

deviance(model1); deviance(model0)
[1] 559.9006
[1] 632.7917
```

### Scale deviance
We can also defined the scale deviance which does not depend on $\phi$:

$$D^{*} = D/\phi$$

and, in the case in which $\phi$ in known, the LRT can be rewritten as the difference in deviances between two models

$$D(\mathcal{M}_{1}) − D(\mathcal{M}_{2}) = 2[I(\hat{\beta}^{\mathcal{M}_{2}}) - I(\hat{\beta}^{\mathcal{M}_{1}})]$$

This is no longer true when $\phi$ needs to be estimated (for example, in a gaussian or gamma distribution)

## Goodness of fit
LRT only holds for nested models, how can we compare non-nested models?

We need general purpose measures of goodness of fit. We use AIC (and BIC) with the same interpretation as for linear models:

```R
AIC(model1)
[1] 921.1983

2*(-as.numeric(logLik(model1)) + length(model1$coefficients))
[1] 921.1983
```

---

# Logistic regression use - SAheart example
To illustrate the use of logistic regression, we will use the SAheart dataset from the book The Elements of Statistical Learning.

```R
fl <- "http://www-stat.stanford.edu/˜tibs/ElemStatLearn/datasets/SAheart"
SAheart <- read.table(fl, sep=",",head=T,row.names=1)

  sbp tobacco ldl   adiposity   famhist   typea obesity alcohol   age chd
1 160 12.00   5.73  23.11       Present   49    25.30     97.20   52  1
2 144 0.01    4.41  28.61       Absent    55    28.87     2.06    63  1
3 118 0.08    3.48  32.28       Present   52    29.14     3.81    46  0
4 170 7.50    6.41  38.03       Present   51    31.99     24.26   58  1
5 134 13.60   3.50  27.78       Present   60    25.99     57.34   49  1
```

This data comes from a retrospective sample of males in a heart-disease high-risk region of the Western Cape, South Africa.
* The response variable is chd, which indicates whether or not coronary heart disease is present in an individual. (numeric 0 / 1 variable.
* Predictors: various measurements for each individual, many related to heart health. For example sbp, systolic blood pressure, and ldl, low density lipoprotein cholesterol.

First we model the probability of coronary heart disease based on low density lipoprotein cholesterol:

$$log \left( \frac{Pr[chd=1]}{1-Pr[chd=1]} \right) = \beta_{0} + \beta_{1dl}x_{1dl}$$

![probchd](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/probchd.png?raw=TRUE)

### Hypothesis testing and confidence interval
As we would expect as ldl increases, so does the probability of chd. We wish to carry out a test for $H_{0}: \beta_{1dl} = 0$ vs $H_{A}: \beta_{1dl} \neq 0$

We find the test statistic and p-value for the test using the summary function
```R
coef(summary(chd_mod_ldl))
              Estimate    Std. Error  z value     Pr(>|z|)
(Intercept)   -1.9686681  0.27307908  -7.209150   5.630207e-13
ldl           0.2746613   0.05163983  5.318787    1.044615e-07
```

We have a low p-value and reject H0: ldl appears to be a relevant predictor.

Confidence intervals at 95%
```R
confint.default(chd_mod_ldl,level=0.95)
2.5 % 97.5 %
(Intercept) -2.5038933 -1.4334430
ldl 0.1734491 0.3758735
```

---

## Prediction

---

## GLM Residuals