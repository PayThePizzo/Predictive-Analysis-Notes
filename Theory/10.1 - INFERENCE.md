# glm in R
The `glm` function fit a GLM model using the maximum likelihood method. For ordinary usage, the call is the same as for lm, except for one extra argument, family. The default value of family is gaussian, for each family we can specify a link function
* In the Poisson regression case the call looks like `glm(y~x, family = poisson)`
* In the logistic regression case, for example, the call looks like `glm(y~x, family = binomial)`

## glm Bernoulli example
```R
logitout <- glm(yesVote ~ . , data = chileElection, family = binomial)
```
The summary here prints out:
* Deviance Residuals, with the min, max and the quantiles
* Coefficients, the usual table with the estimations and significances.
  * Estimate 
  * Std. Error 
  * `z value` and `Pr(>|z|)`, which are test statistics on the single predictors. They are based on the distribution of the MLE, which is a gaussian distribution.
* A note `Dispersion parameter for binomial family taken to be 1` 
* Null deviance and the DoF, similar to R-Squared
* Residual deviance and the DoF, similar to the F-Statistic
* Number of observations deleted `54 observations deleted due to missingness`
* AIC
* Number of Fisher Scoring iterations

### Link
As a default R uses the canonical link, for the binomial we check `binomial()$link` which returns `"logit"`. The link can be changed - see ?family

### Variance 
Specifying the family implies the specification of the variance function:

```R
binomial()$variance

function (mu)
mu * (1 - mu)
<bytecode: 0x000001ee75be2310>
<environment: 0x000001ee764122d8>
```

### Names
Specifying the family and the link implies a number of relationships:

```R
names(binomial())

[1] "family" "link" "linkfun" "linkinv" "variance"
[6] "dev.resids" "aic" "mu.eta" "initialize" "validmu"
[11] "valideta" "simulate"
```

---

## Fitting issues for the logistisc regression
We should note that, if there exists some $\beta^{*}$ such that $X\beta > 0 \Rightarrow y_{i}=1$ and $X\beta < 0 \Rightarrow y_{i}=0$, for all observations, then the MLE is not unique. Which is usually what happens when we have binary response variable.

![separabledata](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/resources/Separabledataglm.png?raw=TRUE)

Such data is said to be separable. There are many ways to proceed with the estimations. 

This, and similar numeric issues related to estimated probabilities near 0 or 1, will return a warning in R.

```R
Warning messages:
1: glm.fit: algorithm did not converge
2: glm.fit: fitted probabilities numerically 0 or 1 occurred
```

When this happens, the model is still “fit”, but there are consequences, namely, the estimated coefficients are highly suspect.
* This is an issue when then trying to interpret the model.
* However it could be useful for creating a classifier

---

## Inference for model parameters

The assumptions on which a generalized linear model is constructed allow us to specify what is the asymptotic distribution of the random vector $\hat{\beta}$ through the theory of MLE. 
* The estimators are normally distributed, unbiased and with the lowest possible variance. 
* We assume that the randomness of Y comes only from $(Y |X_{1} = x_{1},..., X_{p−1} = x_{p−1})$ and not from the predictors.

We rewrite the relationship between the expected value of $\mu$ and the linear predictor in matrix form:

$$\eta = g(\mu) = X\beta$$
* $\beta$ is a vector of parameters that we wish to estimate: we estimate them using an algorithm that maximizes the likelihood
* so the beta parameters enjoy optimal (approximate) properties of MLEs

There is an important difference between the inference results for the Gaussian linear model
and for glm:
* In Gaussian linear model the inference is exact. 
  * This is due to the nice properties of the normal, least squares estimation, and linearity. As a consequence, the distributions of the coefficients are perfectly known assuming that the assumptions hold.
* In generalized linear models the inference is **asymptotic**. 
  * This means that the **distributions of the coefficients are unknown except for large sample sizes n**, for which we have approximations. 
  * Keep in mind that in GLM-like models the relationsip between $\mu(x)$ and $x$ is non-linear: we fit a more complex model and this hinders our ability to make inference.

In fact, we can show that:

$$\hat{\beta} \thicksim \mathcal{N}(\beta, \mathcal{I}(\beta)^{-1}), \text{ When } n \rightarrow \infty$$
* This is an asymptotical approximation
  

Where $\mathcal{I}(\beta)$ is the Fisher information matrix:

$$\mathcal{I}(\beta) = \mathbb{E}\left[ -\frac{\partial^{2}l(\beta)}{\partial\beta\partial\beta^{T}} \right]$$
* The ”larger” (large eigenvalues) the matrix is, the more precise the estimation of $\beta$ is, because that results in smaller variances.

As seen before, it turns out that

$$\mathcal{I}(\beta) = \mathbf{X^{T}VX}$$
* $X$, the design matrix
* $\mathbf{V} = diag(V_{1}, ..., V_{n})$ with $V_{i} = \frac{1}{Var[Y_{i}]}(d\mu/d\eta)^{2}$
  * The $\mathbf{V}$ matrix is used as a weight matrix in the IRLS
  * Notice that in the gaussian linear regression (with identity link) $V_{i}$ are a constant for all i
  
The uncertainty for the $\hat{\beta}$ parameters, also depends on the variance of the observed $Y$. $V_{i}$ values for noticeable distributions:

| Regression 	| $V_{i}$                                           	|
|------------	|---------------------------------------------------	|
| Logistic   	| $V_{i} = \frac{exp(X\beta)}{[1+exp(X\beta)]^{2}}$ 	|
| Poisson    	| $V_{i} = exp(X\beta)$                             	|

As we know, the estimation of the beta parameters depends on the variance, which in turn depends on the beta parameters. This can create quite an ugly situation, since we do not know the true beta parameters. 

The inverse of the Fisher information matrix is estimated by plugging in $\hat{\beta}$ into $\mathcal{I}(\beta)^{-1}$, namely $\mathcal{I}(\hat{\beta})^{-1}$.

### In conclusion
1. The estimates are **asymptotically** unbiased. The variance depends on:
   1. Sample size **n**, as n grows the precision of the estimators increases.
   2. Weighted predictor sparsity $(\mathbf{X}^{T}\mathbf{V}^{-1}\mathbf{X})^{-1}$, the more sparse the predictor is (small
eigenvalues of $(\mathbf{X}^{T}\mathbf{V}^{-1}\mathbf{X})^{-1}$), the more precise $\hat{\beta}$ is.
2. The **precision** of $\hat{\beta}$ is affected by the true value of $\beta$, which is “hidden” inside $\mathbf{V}$
   1. This is partially due to the heteroskedasticity of logistic regression and Poisson regression, which implies a dependence of the variance of $Y$ in the predictors, hence in $\beta$.
   2. This contrasts sharply with the linear model, where the precision of the least squares estimator was not affected by the value of the unknown coefficients.
