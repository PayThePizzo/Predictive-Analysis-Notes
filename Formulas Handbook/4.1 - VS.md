# Variable Selection
Find the simplest model to explain the data. Smaller models lead to less variable
inference/prediction and can be more explainable than complex models. 

> Occam’s Razor principle: the smallest model that fits the data is best.


1. Simplest model for best fit: Use any of the criteria discussed before to choose.
2. The most direct approach is called all subsets or best subsets regression: compute the least squares fit for all possible subsets and then choose between them based on some criterion.

However we often can not examine all possible models, since they are $2^{p-1}$ of them; for
example when $p−1 = 30$ there are 1073741824 models!

Instead we need an automated approach that searches through a subset of them. We
discuss some commonly used approaches next

## Forward Stepwise Selection

1. Let $\mathcal{M_{0}}$ denote the null model which contains no predictors.
2. $\forall k=1,...,p-2$
   1. Consider all $p-k$ models that augment the predictors in $$ with one additional predictor.
   2. Choo

## Backward Stepwise Selection

## Stepwise Search

---

## Discussion

---

## Inference after Selection

## Validation Based Selection




