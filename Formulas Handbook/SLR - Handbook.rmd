---
title: "Handbook for Predictive Analysis"
output:
  html_document:
    theme: readable
    toc: yes
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
# wd setting
setwd("C:/Users/PayThePizzo/Projects/PredictiveAnalysisNotes/Labs/lab01/")
```

---

# SLR

Model we are estimating : $\mathbb{E}[Y|X=x] = \beta_{0} + \beta_{1}X_{i} + \varepsilon_{i}$

```{r, include=FALSE}
# df loading
dataset <- read.csv("C:/Users/PayThePizzo/Projects/PredictiveAnalysisNotes/Labs/lab01/penguins.csv")
dataset <- na.omit(dataset) ## same dataset without missing values
```

Let's see how `body_mass_g` can predict `flipper_length_mm`
```{r}
flipper_length <- bodyfat[,c("body_mass_g", "flipper_length_mm")]
plot(flipper_length_mm~body_mass_g, data = dataset)
```

```{r}
# Define the data
# predictor
x <- dataset$body_mass_g
# target
y <- dataset$flipper_length_mm
n <- length(x)
```

## Manual Mode

Sample values
```{r}
# Sample Variance
s2x <- sum((x-mean(x))^2)/n
s2y <- sum((y-mean(y))^2)/n
# Covariance
covxy <- cov(x,y) 
rxy <- cor(x,y)
# Sample Mean
mx <- mean(x)
my <- mean(y)

# Parameters 
(beta1 <- rxy * sqrt(s2y/s2x))
(beta0 <- my - beta1 *mx)

# Estimated Values
yhat <- beta0 +  beta1 * x 

# Empirical MSE
(mse_hat <- sum((y-yhat)^2))
```

## Automatically

```{r}
# Define model
fit <- lm(formula = y ~ x, data = dataset)
summary(fit)
```

Let us analyze the different aspects of the result.

## Residuals
The `Residuals` prints the sample positional statistics of the residuals for the model.

Usually, for normality purposes, we want:
* The mean to be equal to 0, $\mathbb{E}[\varepsilon_{i}] = 0$
* The mean and the median to coincide
* The 1Q and 3Q to be equally distant from the mean (specular)

## Coefficients 



### Estimate 
The `Estimate` prints the sample coefficients for the model.

#### Correlation
Is the correlation between $X$ and $Y$ positive or negative?

$\beta_{1}$, the angular coefficient, dictates the direction of the slope for the regression
* If the estimate for $\beta_{1}$ is **positive**, then there is a positive correlation between X and Y
    + The sign of $\hat{\beta_{1}}$ is the sign of Pearsons's correlation coefficient $r_{xy}$
    + If too strong, it might symbolize that it is not working properly.
* Else, the correlation is negative

### Std. error

### t-value e p-value

`t value` represents the test statistics: $t_{obs} = \frac{\hat{\beta_{j}}-\beta^{*}}{SE[\hat{\beta_{j}}]} \thicksim t_{n-k}$
* $n = length(x)$
* $k$ is the number of parameters estimated 
* where $\beta^{*} = 0$

`Pr(>|t|)` is the p-value of the test:
* $H_{0}: \beta_{j} = \beta^{*}$
* $H_{1}: \beta_{j} \neq \beta^{*}$
* where $\beta^{*} = 0$

By default they null hypothesis is that the influence of the $\b_{j}$ is 0. In this
case the test looks like this

$t_{obs} = \frac{\hat{\beta_{1}}-0}{SE[\hat{\beta_{1}}]} \thicksim t_{333-2}$

```{r}
sigma2_hat <- (1/n)*mse_hat
s2e <- (n/(n-2))*sigma2_hat
se_beta1 <- sqrt(s2e)/sqrt(n*s2x)

# By hand
(t_obs <- ((beta1-0)/se_beta1))
# Automatically
((fit$coefficients[2]-0)/summary(fit)$coefficients[2,2])
```


If we want to change the $\beta^{*}=1$, the test changes too:

```{r}
# Beta star = 1
sigma2_hat <- (1/n)*mse_hat
s2e <- (n/(n-2))*sigma2_hat
se_beta1 <- sqrt(s2e)/sqrt(n*s2x)

# By hand 
(t_obs <- ((beta1-1)/se_beta1))
# Automatically
((fit$coefficients[2]-1)/summary(fit)$coefficients[2,2])
```

---


