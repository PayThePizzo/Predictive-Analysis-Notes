---
title: "Formulas Handbook"
author: "Gianmaria Pizzo"
output:
  html_document:
    fig_caption: yes
    theme: flatly
    highlight: pygments
    code_folding: show
    toc: yes
    toc_depth: 1
    number_sections: yes
    toc_float:
      smooth_scroll: no
---

Get some more context by looking up the more complete version at [Handbook Formulas for Predictive Analysis](https://github.com/PayThePizzo/Predictive-Analysis-Notes/blob/main/Formulas%20Handbook/Analisi_Predittiva.pdf)

# Setting Up, Basics and Plots

Make sure to set the working directory properly. You can even create the file inside the folder to reach datasets from other sources. 
```{r, eval=FALSE}
# setwd()

# getwd()
```

Import dataset
```{r}
# mrents <- read.table("rent99.raw", header = TRUE)
pengs <- read.csv("C:/Users/PayThePizzo/Projects/PredictiveAnalysisNotes/Labs/lab01/penguins.csv")
```


Show a summary of the data

* Returns `Min.`, `1st Qu.`, `Median`, `Mean`, `3rd Qu.`, `Max`, `NA's` for numerical variables
* Returns `Length`, `Class`, `Mode` for categorical variables

```{r, eval=FALSE}
summary(pengs)
```

Remove all missing values

```{r}
pengs <- na.omit(pengs)
```


## Plots

Quick scatterplot
```{r}
scatterplot <- function(dataset, x, y){
    plot(dataset[, c(x,y)])
}

scatterplot(pengs, "flipper_length_mm", "body_mass_g")
```

Histogram
```{r}
#hist(mrents$rent, xlab = "Rent")
#abline(v=mean(mrents$rent), col = 2, lwd =2)
#abline(v=median(mrents$rent), col = 4, lwd =2)
#legend("topright", bty = "n", col = c(2,4),lwd = 2, legend = c("mean","median"))
```

Barplot
```{r}
#par(mfrow=c(1,2))
#barplot(table(mrents$kitchen)) 
#barplot(table(mrents$cheating))
```

Boxplot

```{r}
#par(mfrow=c(1,2))
#boxplot(rent~kitchen, data = mrents)
#boxplot(rent~bath, data = mrents)
```

```{r}
#mrents$kitchen_level <- factor(mrents$kitchen, labels = c("Standard","Premium"))
#boxplot(rent~kitchen_level, data = mrents, col = c(rgb(0.45,0,0,0.4),rgb(0.118,0.565,1,0.4)),
#        border= c("darkred","dodgerblue"))
```

Plot
```{r}
#plot(rent~area, data = mrents, pch =16, bty = "l",col="darkorange2")
#title(main = "relazione tra area e affitto")
```


## Some Statistics

```{r}

```

---

# SLR - Least Squares

$$Y = \beta_{0} + \beta_{1}X + \epsilon$$
In R we use: `fit <- lm(formula = target ~ predictor, data = dataset)`

```{r}
# If not done before
# pengs <- na.omit(pengs)

fit <- lm(flipper_length_mm ~ body_mass_g , data = pengs)
```

Manually we estimate the coefficients through:

$$\hat{\beta}_1 = \frac{c_{XY}}{s^2_{X}} =  r_{XY} \frac{s_{Y}}{s_{X}},\quad \mbox{and} \quad \hat{\beta}_0 = \overline{y}-\hat{\beta}_1 \overline{x}$$

```{r}
empirical_variance<- function(x, n){
    sum((x-mean(x))^2)/n
}


slr_lse_coefficients <- function(predictor , target){

    # Define basic data
    x <- predictor
    y <- target
    n <- length(x)
    
    # Find plugin variance
    s2x <- empirical_variance(x,n)
    s2y <- empirical_variance(y,n)

    rxy <- cor(x,y)

    mx <- mean(x)
    my<- mean(y)

    # Angular coefficient
    # Dictates how the y changes in proportion to x
    beta1_hat <- rxy * sqrt(s2y/s2x)
    # Alternatively
    # covxy <- cov(x,y) 
    # beta1_hat <- covxy/s2x
    
    # Intercept
    # Forces the regression to pass by the sample mean of x and y.
    beta0_hat <- my - beta1_hat *mx
    # Alternatively
    # beta0 _hat <- se * sqrt (1/n + mean (x) ^2/(n * s2x ))

    
    # Estimated values
    yhat <- beta0_hat + beta1_hat * x
    
    # Empirical MSE
    mse_hat<-(sum((y-yhat)^2))
    
    # Residuals
    res_hat <- y-yhat
    
    # Std. Error
    se_hat <- sqrt(sum((yhat - y)^2)/(n-2))
    
    c(beta0_hat, beta1_hat, yhat, mse_hat, res_hat, se_hat)
}

```

The result will not change:
```{r}
c(slr_lse_coefficients(pengs$body_mass_g, pengs$flipper_length_mm)[1], coefficients(fit)[1])
c(slr_lse_coefficients(pengs$body_mass_g, pengs$flipper_length_mm)[2], coefficients(fit)[2])
```

### Visualize estimated model

```{r}
scatterplot_slr_model <- function(dataset, target, predictor, xlabel, ylabel, title=NULL){
    # Plot 
    dataset <- na.omit(dataset)
    # targe is y, predictor is x
    plot(target ~ predictor, data = dataset,
         xlab=xlabel, ylab=ylabel, main=title)
    # Fit model
    fit <- lm(formula = target ~ predictor, data = dataset)
    # Extract B0
    beta0_hat <- coefficients(fit)[1]
    # Extract B1
    beta1_hat <- coefficients(fit)[2]
    # Plot SLR
    abline(beta0_hat, beta1_hat, col = 2, lwd = 1.4)
}

scatterplot_slr_model(pengs, pengs$flipper_length_mm, pengs$body_mass_g, ylabel = "Flipper Length in mm", xlabel = "Body Mass in grams", title = "flipper_length_mm vs body_mass_g")
```


##

This is considered interpolation as 3420 is not an observed value of x. (But is in the data
range.)
An estimation for the mean of the flipper of a penguin who weights 3420gr
```{r}
# beta_0_hat + beta_1_hat * 3420
coefficients(fit)[1] + coefficients(fit)[2] * 3420
```

This is considered extrapolation as 7500 is not an observed value of x and is outside data
range.

```{r}
# beta_0_hat + beta_1_hat * 3420
coefficients(fit)[1] + coefficients(fit)[2] * 7500
```

### 


## Residuals

## Goodness of fit

### Sum of Squares decomposition

### $R^{2}$ the coefficient of determination

### Plot of the residuals against the predictor X and other variables


---

## SLR - Gaussian Noise

$$\widehat{m}(x) \sim \mathcal{N}\left(\beta_{0}+\beta_{1}x, \sigma^{2}\left( \frac{1}{n}+\frac{(x-\bar{x})^2}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2} \right) \right) $$

$$\downarrow$$

$$\frac{\widehat{m}(x)-Y}{SE[\widehat{m}(x)]} \sim t_{n-2}$$

$$\downarrow$$

$$SE[\widehat{m}(x)] = s_{e}\sqrt{\frac{1}{n} + \frac{(x-\bar{x})^2}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2}}$$


```{r, eval=FALSE}
fit <- lm(formula = target ~ predictor, data = dataset)
```

$$\widehat{\beta}_{0} = \sum_{i=1}^{n}w_{i}^{*}Y_{i} = \bar{Y} - \widehat{\beta}_{1}\bar{x} \sim \mathcal{N}\left(\beta_{0},\sigma^2 \left( \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2}\right) \right)$$

$$\downarrow$$

$$\frac{\widehat{\beta}_{0}-\beta_{0}}{SE[\widehat{\beta}_{0}]} \sim t_{n-2}$$

$$\downarrow$$

$$SE[\widehat{\beta}_{0}] = s_{e}\sqrt{\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2}}$$

$$\widehat{\beta}_{1} = \sum_{i=1}^{n}w_{i}Y_{i} = \frac{\sum_{i=1}^{n}(x_{i}-\bar{x})Y_{i}}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2} \sim \mathcal{N}\left(\beta_{1}, \frac{\sigma^2}{\sum_{i=1}^{n} (x_{i}-\bar{x})^2 } \right)$$

$$\downarrow$$

$$\frac{\widehat{\beta}_{1}-\beta_{1}}{SE[\widehat{\beta}_{1}]} \sim t_{n-2}$$

$$\downarrow$$

$$SE[\widehat{\beta}_{1}] = s_{e}\frac{1}{\sqrt{ \sum_{i=1}^{n}(x_{i}-\bar{x})^2}}$$

$$s_e^2 = \frac{\sum_{i=1}^{n}(y_{i} - \hat{y}_{i})^2}{n-2} $$

```{r}
#x <- pengs$body_mass_g
#y <- pengs$flipper_length_mm
#n <- length(x)

# <- sum((x-mean(x))^2)/n
#s2y <- sum((y-mean(y))^2)/n

#covxy <- cov(x,y) 
#rxy <- cor(x,y)

#mx <- mean(x)
#my<- mean(y)

# Angular coefficient
# Dictates how the y changes in proportion to x
#beta1_hat <- rxy * sqrt(s2y/s2x)
# Intercept
# Forces the regression to pass by the sample mean of x and y.
#beta0_hat <- my - beta1_hat *mx

# The coefficients
#c(beta0_hat, beta1_hat)
```

```{r}
computeSumSquares <- function(bs, y, x){
  observed <- y
  modelled <- bs[1] + bs[2] * x
  squares <- (observed - modelled)^2
  sum(squares)
}
```

##

## Inference

### Confidence Intervals

### Prediction Intervals

### Hypothesis Testing


---

# MLR

$$$$

```{r, eval=FALSE}
##lm - modelli lineari  -------
df <- data.frame(x1 = runif(15), x2 = runif(15), y  = rnorm(15))

## stima del modello 
fit <- lm(y~x1+x2, data = df)
summary(fit) ## varie informazioni riassuntive sulla stima
coef(fit) ## valori stimati dei coefficienti del modello 
confint(fit) ## intervalli di confidenza per i coefficienti del modello 

## per aggiungere trasformazioni di X come predittori si usa la funzione I(.)
## fit <- lm(y~x1+x2+I(x2^2), data = df) 
## per polinomi 
## fit <- lm(y~x1+poly(x2,2), data = df)
## fit <- lm(y~x1+poly(x2,2, raw=TRUE), data = df)

## la matrice di disegno usata nella stima
model.matrix(fit)

## predizione 
# per i valori osservati delle X
fitted(fit)
predict(fit) ## predict produce anche intervalli di confidenza e predizione
predict(fit, interval = "confidence") 
predict(fit, interval = "prediction") 
## per un nuovo set di punti 
nd <- data.frame(x1 = c(0.2,0.8), x2 = c(0.3,0.6))
predict(fit, newdata = nd)

# residui 
residuals(fit) ## these are y - fitted(fit)
rstandard(fit) ## standardised residuals 
rstudent(fit)  ## studentized residuals 

# goodness of fit/ bontC  di adattamento 
plot(fit) # grafici riassuntivi
AIC(fit, k = 2); BIC(fit); logLik(fit) ## verosimiglianza e criteri di informazione
hatvalues(fit) ##  leverages - punti di leva 
car::vif(fit) ## variance inflation factors - ci sono problemi di colinearitC ? 
cooks.distance(fit) # outliers/punti particolari 

## test anova per modelli annidati 
# anova(small_model, big_model)
anova(lm(y~x1, data = df), fit)


## model selection 
## la funzione step qui usata per un algoritmo forward come esempio 
## opzioni importanti 
# scope per delineare l'ambito della ricerca
# k: per definire la penalizzazione del criterio di informazione
# direction: per la direzione: forward, backward, both

step(object = lm(y~1, data = df), 
     scope = list(lower = lm(y~1, data = df), 
                  upper = fit), 
     direction = "forward",
     k = 2
) ## 


## trasformazione di Box-Cox 
## da usare se y|X risulta non-normale 
## MASS::boxcox

```

---

# GLM

$$$$

```{r, eval=FALSE}
##glm - modelli lineari generalizzati -------

##in questo esempio usiamo una Poisson 

df <- data.frame(x1 = runif(15), x2 = runif(15), y  = rpois(15, 6))

## stima del modello 
fit <- glm(y~x1+x2, data = df, family = poisson()) 
## di default si usa la funzione legame canonica
poisson()$link
summary(fit) ## varie informazioni riassuntive sulla stima
coef(fit) ## valori stimati dei coefficienti del modello 
confint.default(fit) ## intervalli di confidenza per i coefficienti del modello 

## predizione 
# per i valori osservati delle X
fitted(fit) ## predizione sulla scale di Y (exp(linear.predictor))
predict(fit) ## predict di default mostra il predittore lineare
predict(fit, type = "response") ## predict accetta un'opzione type per mostrare i valori stimati sulla scala delle Y 
## per un oggetto glm predict non puC2 costruire intervalli di confidenza (e non si possono costruire intervalli di predizione)
predict(fit, se.fit = TRUE) # con opzione se.fit si ottiene lo standard error per il predittore lineare 
## per un nuovo set di punti 
nd <- data.frame(x1 = c(0.2,0.8), x2 = c(0.3,0.6))
a <- predict(fit, newdata = nd, se.fit = TRUE); a
# intervalli di confidenza manuali
alpha = 0.05
cbind(a$fit + qnorm(alpha/2) * a$se.fit, 
      a$fit + qnorm(1-alpha/2) * a$se.fit)
      
        
# residui 
residuals(fit) ## di default deviance residuals 
residuals(fit, type = "pearson") ## type = c("deviance", "pearson", "response"))

# goodness of fit/ bontC  di adattamento 
plot(fit) # grafici riassuntivi
AIC(fit, k = 2); BIC(fit); logLik(fit) ## verosimiglianza e criteri di informazione

## test anova per modelli annidati 
# anova(small_model, big_model)
anova(glm(y~x1, data = df, family=poisson()), fit, test = "LRT")



## glm as a classifier ------------- 

## funzioni implementate nelle slides/laboratorio 


cv_class <- function(K=5, dat, model, cutoff = 0.5){
  assign_group <- rep(seq(1,K), each = floor(nrow(dat)/K))
  ### this ensures we use all points in the dataset
  ### this way we might have subgroups of different size 
  if(length(assign_group) != nrow(dat)) assign_group <- c(assign_group, sample(seq(1,K)))[1:nrow(dat)] 
  assign_group <- sample(assign_group, size = nrow(dat))
  error <- 0
  for(j in 1:K){
    whichobs <- (assign_group == j)
    ## fit a model WITHOUT the hold-out data
    folded_model <- suppressWarnings(glm(model$formula, 
                                         data = dat[!whichobs,], 
                                         family = "binomial"))
    ## evaluate the model on the hold-out data
    fitted <- suppressWarnings(predict(folded_model,
                                       dat[whichobs,], 
                                       type="response"))
    observed <- dat[whichobs, strsplit(paste(model$formula), "~")[[2]]]
    error <- error + mean(observed != (fitted>cutoff))/K 
    ### in cv.glm the actual error is calculated as (y - p(y=1)) 
    # error <- error + mean((observed - fitted)^2)/K 
    ### the mis-classification rate will depend on how we decide what is assigned to each category 
  }
  error
}


make_conf_mat <- function(predicted, actual) {
  table(predicted = predicted, actual = actual)
}

get_sens <- function(conf_mat) {
  conf_mat[2, 2] / sum(conf_mat[, 2])
}
# Note that this function is good for illustrative purposes, but is easily broken. (Think about what happens if there are no "positives" predicted.)


get_spec <-  function(conf_mat) {
  conf_mat[1, 1] / sum(conf_mat[, 1])
}
```

---

## GLM - Classifier


