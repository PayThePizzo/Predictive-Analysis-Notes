---
title: "Formulas Handbook"
author: "Gianmaria Pizzo"
output:
  html_document:
    fig_caption: yes
    theme: flatly
    highlight: pygments
    code_folding: show
    toc: yes
    toc_depth: 1
    number_sections: yes
    toc_float:
      smooth_scroll: no
---

# Setting Up, Basics and Plots

Make sure to set the working directory properly. You can even create the file inside the folder to reach datasets from other sources. 
```{r, eval=FALSE}
# setwd()
# getwd()
```

Import dataset
```{r}
# mrents <- read.table("rent99.raw", header = TRUE)
pengs <- read.csv("C:/Users/PayThePizzo/Projects/PredictiveAnalysisNotes/Labs/lab01/penguins.csv")
```

Show a summary of the data

* Returns `Min.`, `1st Qu.`, `Median`, `Mean`, `3rd Qu.`, `Max`, `NA's` for numerical variables
* Returns `Length`, `Class`, `Mode` for categorical variables

```{r, eval=FALSE}
summary(pengs)
```

Remove all missing values

```{r}
pengs <- na.omit(pengs)
```


## Plots

Quick scatterplot
```{r}
scatterplot <- function(dataset, x, y){
    plot(dataset[, c(x,y)])
}

scatterplot(pengs, "flipper_length_mm", "body_mass_g")
```

Histogram
```{r}
#hist(mrents$rent, xlab = "Rent")
#abline(v=mean(mrents$rent), col = 2, lwd =2)
#abline(v=median(mrents$rent), col = 4, lwd =2)
#legend("topright", bty = "n", col = c(2,4),lwd = 2, legend = c("mean","median"))
```

Barplot
```{r}
#par(mfrow=c(1,2))
#barplot(table(mrents$kitchen)) 
#barplot(table(mrents$cheating))
```

Boxplot

```{r}
#par(mfrow=c(1,2))
#boxplot(rent~kitchen, data = mrents)
#boxplot(rent~bath, data = mrents)
```

```{r}
#mrents$kitchen_level <- factor(mrents$kitchen, labels = c("Standard","Premium"))
#boxplot(rent~kitchen_level, data = mrents, col = c(rgb(0.45,0,0,0.4),rgb(0.118,0.565,1,0.4)),
#        border= c("darkred","dodgerblue"))
```

Plot
```{r}
#plot(rent~area, data = mrents, pch =16, bty = "l",col="darkorange2")
#title(main = "relazione tra area e affitto")
```


## Some Statistics

```{r}

```

---

# SLR - Least Squares

$$Y = \beta_{0} + \beta_{1}X + \epsilon$$

```{r}
# If not done before
# pengs <- na.omit(pengs)
fit <- lm(flipper_length_mm ~ body_mass_g , data = pengs)
```

## Sampling

Manually we estimate the coefficients through:

$$\hat{\beta}_1 = \frac{c_{XY}}{s^2_{X}} =  r_{XY} \frac{s_{Y}}{s_{X}},\quad \mbox{and} \quad \hat{\beta}_0 = \overline{y}-\hat{\beta}_1 \overline{x}$$

```{r}
empirical_variance<- function(x, n){
    sum((x-mean(x))^2)/n
}


slr_lse_coefficients <- function(predictor , target){

    # Define basic data
    x <- predictor
    y <- target
    n <- length(x)
    
    # Find plugin variance
    s2x <- empirical_variance(x,n)
    s2y <- empirical_variance(y,n)

    rxy <- cor(x,y)

    mx <- mean(x)
    my<- mean(y)

    # Angular coefficient
    # Dictates how the y changes in proportion to x
    beta1_hat <- rxy * sqrt(s2y/s2x)
    # Alternatively
    # covxy <- cov(x,y) 
    # beta1_hat <- covxy/s2x
    
    # Intercept
    # Forces the regression to pass by the sample mean of x and y.
    beta0_hat <- my - beta1_hat *mx
    # Alternatively
    # beta0 _hat <- se * sqrt (1/n + mean (x) ^2/(n * s2x ))

    
    # Estimated values
    yhat <- beta0_hat + beta1_hat * x
    
    # Empirical MSE
    mse_hat<-(sum((y-yhat)^2))
    
    # Residuals
    res_hat <- y-yhat
    
    # Std. Error
    se_hat <- sqrt(sum((yhat - y)^2)/(n-2))
    
    c(beta0_hat, beta1_hat, yhat, mse_hat, res_hat, se_hat)
}

```

The result will not change:
```{r}
c(slr_lse_coefficients(pengs$body_mass_g, pengs$flipper_length_mm)[1], coefficients(fit)[1])
c(slr_lse_coefficients(pengs$body_mass_g, pengs$flipper_length_mm)[2], coefficients(fit)[2])
```

### Visualize estimated model

```{r}
scatterplot_slr_model <- function(dataset, target, predictor, xlabel, ylabel, title=NULL){
    # Plot 
    dataset <- na.omit(dataset)
    # targe is y, predictor is x
    plot(target ~ predictor, data = dataset,
         xlab=xlabel, ylab=ylabel, main=title)
    # Fit model
    fit <- lm(formula = target ~ predictor, data = dataset)
    # Extract B0
    beta0_hat <- coefficients(fit)[1]
    # Extract B1
    beta1_hat <- coefficients(fit)[2]
    # Plot SLR
    abline(beta0_hat, beta1_hat, col = 2, lwd = 1.4)
}

scatterplot_slr_model(pengs, pengs$flipper_length_mm, pengs$body_mass_g, ylabel = "Flipper Length in mm", xlabel = "Body Mass in grams", title = "flipper_length_mm vs body_mass_g")
```


## Inference

Gli intervalli di confidenza danno un'indicazione dell'incertezza attorno al valore medio del tempo passato da ogni utente nell'app; gli intervalli di predizione invece danno un'indicazione dell'incertezza per quelli che possono essere gli effettivi valori del tempo passato da un utente nell'app. Dato che la media è per definizione meno variabile delle singole osservazioni gli intervalli di confidenza sono meno ampi degli intervalli di predizione.

## Prediction

This is considered interpolation as 3420 is not an observed value of x. (But is in the data
range.)
An estimation for the mean of the flipper of a penguin who weights 3420gr
```{r}
# beta_0_hat + beta_1_hat * 3420
coefficients(fit)[1] + coefficients(fit)[2] * 3420
```

This is considered extrapolation as 7500 is not an observed value of x and is outside data
range.

```{r}
# beta_0_hat + beta_1_hat * 3420
coefficients(fit)[1] + coefficients(fit)[2] * 7500
```

### 

## Residuals

```{r, eval = FALSE}

```

## Goodness of fit

### Sum of Squares decomposition

### $R^{2}$ the coefficient of determination

### Plot of the residuals against the predictor X and other variables


---

# SLR - Gaussian Noise

The model is briefly specified as follows (these are the assumptions):

\[ Y_i|X=x_i \stackrel{iid}{\sim} N(\beta_0 + \beta_1 x_i, \sigma) \quad \quad \forall i= 1, \ldots, n \]. 

or

\[ Y = \beta_0 + \beta_1 x_i + \epsilon_{i} \quad \quad \forall i= 1, \ldots, n \]. 

with $\epsilon \thicksim \mathcal{N}(0, \sigma^2)$ (independent errors)

```{r, eval=FALSE}
fit <- lm(formula = target ~ predictor, data = dataset)
```



$$\widehat{m}(x) \sim \mathcal{N}\left(\beta_{0}+\beta_{1}x, \sigma^{2}\left( \frac{1}{n}+\frac{(x-\bar{x})^2}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2} \right) \right) $$

$$\downarrow$$

$$\frac{\widehat{m}(x)-Y}{SE[\widehat{m}(x)]} \sim t_{n-2}$$

$$\downarrow$$

$$SE[\widehat{m}(x)] = s_{e}\sqrt{\frac{1}{n} + \frac{(x-\bar{x})^2}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2}}$$

```{r, eval=FALSE}
fit <- lm(formula = target ~ predictor, data = dataset)
```

$$\widehat{\beta}_{0} = \sum_{i=1}^{n}w_{i}^{*}Y_{i} = \bar{Y} - \widehat{\beta}_{1}\bar{x} \sim \mathcal{N}\left(\beta_{0},\sigma^2 \left( \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2}\right) \right)$$

$$\downarrow$$

$$\frac{\widehat{\beta}_{0}-\beta_{0}}{SE[\widehat{\beta}_{0}]} \sim t_{n-2}$$

$$\downarrow$$

$$SE[\widehat{\beta}_{0}] = s_{e}\sqrt{\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2}}$$

$$\widehat{\beta}_{1} = \sum_{i=1}^{n}w_{i}Y_{i} = \frac{\sum_{i=1}^{n}(x_{i}-\bar{x})Y_{i}}{\sum_{i=1}^{n}(x_{i}-\bar{x})^2} \sim \mathcal{N}\left(\beta_{1}, \frac{\sigma^2}{\sum_{i=1}^{n} (x_{i}-\bar{x})^2 } \right)$$

$$\downarrow$$

$$\frac{\widehat{\beta}_{1}-\beta_{1}}{SE[\widehat{\beta}_{1}]} \sim t_{n-2}$$

$$\downarrow$$

$$SE[\widehat{\beta}_{1}] = s_{e}\frac{1}{\sqrt{ \sum_{i=1}^{n}(x_{i}-\bar{x})^2}}$$

$$s_e^2 = \frac{\sum_{i=1}^{n}(y_{i} - \hat{y}_{i})^2}{n-2} $$

```{r}
#x <- pengs$body_mass_g
#y <- pengs$flipper_length_mm
#n <- length(x)

#s2x <- sum((x-mean(x))^2)/n
#s2y <- sum((y-mean(y))^2)/n

#covxy <- cov(x,y) 
#rxy <- cor(x,y)

#mx <- mean(x)
#my<- mean(y)

# Angular coefficient
# Dictates how the y changes in proportion to x
#beta1_hat <- rxy * sqrt(s2y/s2x)
# Intercept
# Forces the regression to pass by the sample mean of x and y.
#beta0_hat <- my - beta1_hat *mx

# The coefficients
#c(beta0_hat, beta1_hat)
```

```{r}
computeSumSquares <- function(bs, y, x){
  observed <- y
  modelled <- bs[1] + bs[2] * x
  squares <- (observed - modelled)^2
  sum(squares)
}
```

## Inference

### Confidence Intervals

For some beta

```{r}
# EST + CRIT * sigma
confint_samp <- function(est, alpha, sigma_est, n_rows, predictors){
    est + qt(c(0.0 + (alpha/2), 1.0 - (alpha/2)), df=(n_rows-predictors)) * sigma_est
}

eval_hyp <- function(est, hyp, sigma_est, n_rows, predictors, alpha){
    ts <- (est-hyp)/sigma_est
    interval <- qt(c(0.0 + (alpha/2), 1.0 - (alpha/2)),
                   df=(n_rows-predictors))
    data.table::between(ts,
                    interval[1],
                    interval[2])
}

eval_hyp(1.5, 3, 2, 10, 2, alpha=0.05)

```

### Prediction Intervals

### Hypothesis Testing


---

# MLR

The model is specified as follows (these are the assumptions):

\[ target_{i} = \beta_{0} + \beta_{1}first_{i} + \beta_{2}second_{i} + \ldots + \beta_{p-1}pth + \epsilon_{i} \quad \quad \forall i=1,\ldots,n \]

with $\epsilon_{i} \thicksim \mathcal{N}(0, \sigma^{2})$ (independent errors).

```{r, eval= FALSE}
fit <- lm(target~predictor1+predictor2, data = df)

## la matrice di disegno usata nella stima
model.matrix(fit)
```

## Sampling

```{r, eval=FALSE}
##lm - modelli lineari  -------
df <- data.frame(x1 = runif(15), x2 = runif(15), y  = rnorm(15))

## stima del modello 
fit <- lm(y~x1+x2, data = df)
summary(fit) ## varie informazioni riassuntive sulla stima
coef(fit) ## valori stimati dei coefficienti del modello 
confint(fit) ## intervalli di confidenza per i coefficienti del modello 

## per aggiungere trasformazioni di X come predittori si usa la funzione I(.)
## fit <- lm(y~x1+x2+I(x2^2), data = df) 
## per polinomi 
## fit <- lm(y~x1+poly(x2,2), data = df)
## fit <- lm(y~x1+poly(x2,2, raw=TRUE), data = df)

## la matrice di disegno usata nella stima
model.matrix(fit)

## predizione 
# per i valori osservati delle X
fitted(fit)
predict(fit) ## predict produce anche intervalli di confidenza e predizione
predict(fit, interval = "confidence") 
predict(fit, interval = "prediction") 
## per un nuovo set di punti 
nd <- data.frame(x1 = c(0.2,0.8), x2 = c(0.3,0.6))
predict(fit, newdata = nd)

# residui 
residuals(fit) ## these are y - fitted(fit)
rstandard(fit) ## standardised residuals 
rstudent(fit)  ## studentized residuals 

# goodness of fit/ bontC  di adattamento 
plot(fit) # grafici riassuntivi
AIC(fit, k = 2); BIC(fit); logLik(fit) ## verosimiglianza e criteri di informazione
hatvalues(fit) ##  leverages - punti di leva 
car::vif(fit) ## variance inflation factors - ci sono problemi di colinearitC ? 
cooks.distance(fit) # outliers/punti particolari 

## test anova per modelli annidati 
# anova(small_model, big_model)
anova(lm(y~x1, data = df), fit)


## model selection 
## la funzione step qui usata per un algoritmo forward come esempio 
## opzioni importanti 
# scope per delineare l'ambito della ricerca
# k: per definire la penalizzazione del criterio di informazione
# direction: per la direzione: forward, backward, both



## trasformazione di Box-Cox 
## da usare se y|X risulta non-normale 
## MASS::boxcox

```

## Residuals

```{r, eval=FALSE}
# residui 
residuals(fit) ## these are y - fitted(fit)
rstandard(fit) ## standardised residuals 
rstudent(fit)  ## studentized residuals 
```

## Inference

### Confidence Intervals

### Prediction Intervals

### Hypothesis Testing

## Model Selection

### ANOVA

```{r, eval=FALSE}
## test anova per modelli annidati 
# anova(small_model, big_model)
anova(fit_small, fit_big)
```

Large F statistic -> reject H_{0}

## Quality Criterion

```{r,eval=FALSE}
logLik(fit)
```

### AIC

$$-2logLik(M) + 2p(M)$$
```{r, eval=FALSE}
AIC(fit, k=2)

# or
AIC_man <- function(loglikelihood, p){
    ((-2*loglikelihood)+(2*p))
}
```

### BIC

$$-2logLik(M)+ log(n)p(M)$$
```{r, eval=FALSE}
AIC(fit, k=log(n))

BIC(fit)

BIC_man <- function(loglikelihood, n, p){
    ((-2*loglikelihood)+(log(n)*p))
}
```


### Adjusted $R^{2}$

$$\text{Adjusted }R^{2} = 1 - \frac{SS_{RES}/(n-p-1)}{SS_{TOT}/(n-1)}$$
```{r, eval=FALSE}
summary(fit)$adj.r.squared

adj_r_squared <-function(y, y_hat, n,p){
    (sum((y-y_hat)**2)/(n-p-1))/(sum((y-mean(y))**2)/(n-1))
}
```

## Variable Selection

### Forward Stepwise Selection

```{r, eval=FALSE}
# Define null and full model

# FS - AIC
step(null, 
     scope=list(lower=null, upper=full), 
     direction="forward", k=2, trace=1)

# FS - BIC
step(null, scope=list(lower=null, upper=full), 
     direction="forward", k=log(n), trace=1)
```

### Backward Stepwise Selection

```{r, eval=FALSE}
# Define null and full model

# BS - AIC
step(full, scope=list(lower=null, upper=full), 
     direction="backward", k=2, trace=1)
# BS - BIC
step(full, scope=list(lower=null, upper=full), 
     direction="backward", k=log(n), trace=1)
```


### Stepwise Selection (Both)

```{r, eval=FALSE}
# Define null, intermediate and full model

# Step - AIC
step(intermediate, 
     scope = list(lower = null, upper=full),
     direction="both", trace=1, k=2)

# Step - BIC
step(intermediate, 
     scope = list(lower = null, upper=full),
     direction="both", trace=1, k=log(n))
```

### LOOCV

```{r, eval=FALSE}

calc_loocv_rmse = function(model) {
    sqrt(mean((resid(model) / (1 - hatvalues(model))) ˆ 2))
}

```

---

## Categorical Predictors

## Interactions

## Categorical Predictors and Interactions

--- 

# Model Checking - LINE 

The assumptions for the model can be briefly written as:

* Linearity: The response can be written as a linear combination of the predictors (With some nois about this true linear relationship);
* Independence: The errors and the observations are independent;
* Normality: The distribution of the errors $\epsilon \thicksim \mathcal{N}(0, \sigma^2)$;
* Equal Variance: Homoskedasticity, the error variance is the same at any set of predictor values.

These assumptions are typically verified through the study of the residuals.

Queste assunzioni vengono tipicamente verificate tramite lo studio dei residui. Dalle informazioni a nostra disposizione non c'è modo di sapere se le aziende in esame sono indipendenti tra loro. 

Dai grafici dei residui si nota che le altre assunzioni sembrano non essere valide per il modello `fit1`: i residui dimostrano di non essere sparsi in maniera casuale in funzione dei valori stimati, indicando problemi con l'assunzione di linearità della relazione (che già si notava nel grafico fatto al punto b), la varianza cresce al crescere dei valori stimati, le code della distribuzione dei residui sono più pesanti di quelle che ci si aspetterebbe da un campione estratto da una normale. Inoltre ci sono alcuni punti che hanno una forte influenza sulla stima, come evidenziato dal fatto che per alcuni punti la distanza di Cook e molto grande. 


## Linearity
We can check the linearity in many ways

Plot $Y \text{ vs } \beta_{j}$

```{r}

```

## Independence
Hard to test unless specified in the data. We have no way to know whether the observations are independent between them.


## Normality

```{r}

```

## Equal Variance/ Homoskedasticity

```{r}

```

---

# Transformations

## Variance Stabilizing Target Transformations

Applying an invertible function to Y to achieve $Var[g(Y)|X=x] = c$

$$g(Y) = \beta_{0} + \beta_{1}x + \varepsilon_{i} \rightarrow Y = g( \beta_{0} + \beta_{1}x + \varepsilon_{i})^{-1} $$
Usually we apply the logarithm (positive data)

$$log(Y_{i}) = \beta_{0} +\beta_{1}x_{i} + \varepsilon_{i} \rightarrow Y_{i} = exp \left\{ \beta_{0} + \beta_{1}x \right\} \cdot exp \left\{\varepsilon_{i} \right\}$$

```{r, eval=FALSE}
lm(log(target)~predictor, data = df)
```

Which results in a multiplicative model

$$\hat{y_{i}} = \exp\{\hat{\beta_{0}}\} \cdot \exp\{\hat{\beta_{1}} x_{i}\}$$
```{r, eval=FALSE}
plot(target ~ predictor, data = df, 
     col = "grey", pch = 20, cex = 1.5, main = "Title")

# lower, upper are numbers at the extremes of the range
curve(exp(fit$coef[1] + fit$coef[2] * x),
      from = lower, to = upper, add = TRUE, 
      col = "darkorange", lwd = 2)
```

## Box Cox 

```{r, eval=FALSE}
## trasformazione di Box-Cox 
## da usare se y|X risulta non-normale 
MASS::boxcox (y~x, data = df, lambda = seq(-0.5,0.5, by=0.05))

bctransf$x[which.max(bctransf$y)]

# Box-Cox transform
boxcox(fit, plotit = TRUE)
```

## Polynomials

Polynomials are used to make the model more flexible or to fix the violation of some assumptions.


$$Y_{i} = \beta_{0} + \beta_{1}x_{i}+ \beta_{2}x^{2}_{i} + ... + \beta_{K}x^K_{i}+ \epsilon_i$$

```{r, eval= FALSE}
# Even count of polynomials
lm(formula = target ~ predictor + I(predictor^2) + I(predictor^3) + I(predictor^4), data = df)

# Or
lm(formula = target ~ poly(predictor, 4, raw=TRUE), data = df)

# --- 

# Odd count of polynomials
lm(formula = target ~ predictor + I(predictor^2) + I(predictor^3),
   data = df)

# Or
lm(formula = target ~ poly(predictor, 3, raw=TRUE), data = df)
```

---

# Collinearity 

Relationship of dependence involving pairs of predictors

Can be checked through 

* Pairplots
* Correlation Matrix

# Multicollinearity

Relationship of dependence involving 3 or more of predictors

Some signals include

* 

## VIF

```{r, eval=FALSE}
hatvalues(fit) ##  leverages - punti di leva 
car::vif(fit) ## variance inflation factors - ci sono problemi di colinearità? 
cooks.distance(fit) # outliers/punti particolari 
```

---

# Influence



---

# GLM

## Dati Binari

Tipicamente quando vogliamo vedere un predittore contro una risposta, ma la risposta e’ binaria, possiamo “sparpagliare” i valori di 0 e 1 con la funzione jitter. Questa funzione sparpaglia i valori in verticale.

```{r, eval = FALSE}
plot(jitter(ifelse(opposed == "opposed", 1,0), amount = 0.1) ~ year, data = dat)

plot(jitter(ifelse(opposed == "opposed", 1,0), amount = 0.1) ~ jitter(year, amount = 0.2), data = dat)
```

## Poisson

$$Y_{i} = [Y|X=x_{i}] \thicksim Pois(\lambda(X_{i})) \quad \quad \text{per } i= 1, \ldots, n$$
dove $\lambda(X_{i}) = exp\{X\beta\}$

```{r, eval=FALSE}
fit_pois <- glm(target~predictor, 
               data = df, 
               family = poisson())
```

Si descrive come:

 $$\text{target}_i|\text{predictor}_i \sim Pois(\lambda(\text{predictor}_i)) \quad \quad \text{per } i= 1, \ldots, n$$
dove $\lambda(predictor_{i}) = exp \left\{ \beta_{0} + \beta_{1}predictor_{i} \right\}$

```{r, eval=FALSE}
par(pch = 16, bty = "l", mfrow=c(1,2))

nd <- data.frame(yearSince1979 = seq(1, 18, by = 1))

### we can not plot the inverse-link transformed data
### they take values -inf or + inf 
plot(nd$yearSince1979, predict(fit_bin, newdata = nd),
     type="l", main = "The linear predictor")

plot(nd$yearSince1979, predict(fit_bin, newdata = nd, type="response"),
     type="l", ylim = c(-0.25,1.25), main = "The estimated probability")

### We can plot the original data
points(dat$yearSince1979, jitter(dat$numfopp), col = "grey70")
```


```{r, eval=FALSE}
# Linear Predictor Estimate

# Manually with 
# xb = beta0 + beta1 * x

linear_pred <- predict(fit_pois, 
                       newdata = nd, 
                       type = "link")
```

$$\mathbb{E}[Y=1|X=x_{i}] = \mathbb{E}[Yi] = Var(Y_{i}) = exp(X\beta)$$
```{r, eval=FALSE}
# Expected Value Estimate

# xb = beta0 + beta1 * x
glm_pois_mu <-function(xb){
    exp(xb)
}

response_pred <- predict(fit_pois, 
                         newdata = nd, 
                         type = "response")
```

## Binomial

$$[Y|X=x] \thicksim Bin(k, p(x))$$



```{r, eval=FALSE}
fit_glm <- glm(target~predictor, data = df, family = binomial)
```

More precisely, we need to specify the data for the binomial:

```{r, eval=FALSE}

# 1 - Factor
# 'success' is interpreted as the factor not having the first level 
# (and hence usually of having the second level)
# WE NEED TO MAKE SURE THE RIGHT LEVEL IS USED TO REPRESENT SUCCESS
df$target <- as.factor(df$target)

glm(target ~ predictor, data = df, family = binomial)

#--------------------

# 2 - Numerical vector 
# with values between 0 and 1, interpreted as the proportion 
# of successful cases (with the total number of cases given by the weights).

df$numsuccess <- as.numeric(df$target == "success") # 1 when 'success'

glm(numsuccess ~ predictor, data = df, 
    family = binomial, 
    weights = rep(1, nrow(df))) # We need to specify the weights

#--------------------

# 3 - As a two-column integer matrix: 
# the first column gives the number of successes and 
# the second the number of failures.
df$numsuccess <- as.numeric(df$target == "success") # successes
df$ntrial <- rep(1, nrow(df)) # failures

glm(cbind(df$numsuccess, df$ntrial-df$numsuccess) ~ df$predictor, 
    family = binomial)
```


```{r, eval=FALSE}

plot(jitter(Class, amount = 0.15)~Thick, 
     data = wbca, ylab = "P(tumor is benign)")

lines(sort(wbca$Thick), 
      fitted(tumor_thick)[order(wbca$Thick)],col=2)
```



## Bernoulli

$$[Y|X=x] \thicksim Bern(p(x)) \rightarrow [Y|X=x] \thicksim Bin(k=1, p(x))$$

```{r, eval=FALSE}
fit_bern <- glm(target~predictor1, 
                data = df, 
                family = binomial())
```

$$$$

```{r, eval=FALSE}
# Fitted values on link scale

xb <- coef(fit_bern)[1] + coef(fit_bern)[2] * nd$predictor1

linear_pred <- predict(fit_bern, 
                       newdata = nd, 
                       type = "link") #default
```

$$\mathbb{E}[Y=1|X=x] = logit(X\beta)  = \frac{e^{X\beta}}{1-e^{X\beta}}$$

```{r, eval=FALSE}
# Fitted values on response scale

# Manually with 
# xb = beta0 + beta1 * x
glm_bern_mu <- function(xb){
    (exp(xb)/(1+exp(xb)))
}

# Or
# nd <- data.frame(predictor1=c(0.5, ...))

response_pred <- predict(fit_bern, 
                         newdata = nd, 
                         type="response")
```

$$oddsratio$$
```{r}
glm_odds_ratio <- function(xb){
    (xb)
}
```



## Inference and Prediction

```{r, eval=FALSE}
confint.default(fit) ## intervalli di confidenza per i coefficienti del modello 

## predizione 
# per i valori osservati delle X
fitted(fit) ## predizione sulla scale di Y (exp(linear.predictor))
predict(fit) ## predict di default mostra il predittore lineare
predict(fit, type = "response") ## predict accetta un'opzione type per mostrare i valori stimati sulla scala delle Y 
## per un oggetto glm predict non puC2 costruire intervalli di confidenza (e non si possono costruire intervalli di predizione)
predict(fit, se.fit = TRUE) # con opzione se.fit si ottiene lo standard error per il predittore lineare 
## per un nuovo set di punti 
nd <- data.frame(x1 = c(0.2,0.8), x2 = c(0.3,0.6))
a <- predict(fit, newdata = nd, se.fit = TRUE); a
# intervalli di confidenza manuali
alpha = 0.05
cbind(a$fit + qnorm(alpha/2) * a$se.fit, 
      a$fit + qnorm(1-alpha/2) * a$se.fit)
```      

## Residuals

```{r, eval = FALSE}     
# residui 
residuals(fit) ## di default deviance residuals 
residuals(fit, type = "pearson") ## type = c("deviance", "pearson", "response"))

# goodness of fit/ bontC  di adattamento 
plot(fit) # grafici riassuntivi
AIC(fit, k = 2); BIC(fit); logLik(fit) ## verosimiglianza e criteri di informazione

## test anova per modelli annidati 
# anova(small_model, big_model)
anova(glm(y~x1, data = df, family=poisson()), fit, test = "LRT")
```



---

## GLM - Classifier

```{r, eval=FALSE}
## funzioni implementate nelle slides/laboratorio 


cv_class <- function(K=5, dat, model, cutoff = 0.5){
  assign_group <- rep(seq(1,K), each = floor(nrow(dat)/K))
  ### this ensures we use all points in the dataset
  ### this way we might have subgroups of different size 
  if(length(assign_group) != nrow(dat)) assign_group <- c(assign_group, sample(seq(1,K)))[1:nrow(dat)] 
  assign_group <- sample(assign_group, size = nrow(dat))
  error <- 0
  for(j in 1:K){
    whichobs <- (assign_group == j)
    ## fit a model WITHOUT the hold-out data
    folded_model <- suppressWarnings(glm(model$formula, 
                                         data = dat[!whichobs,], 
                                         family = "binomial"))
    ## evaluate the model on the hold-out data
    fitted <- suppressWarnings(predict(folded_model,
                                       dat[whichobs,], 
                                       type="response"))
    observed <- dat[whichobs, strsplit(paste(model$formula), "~")[[2]]]
    error <- error + mean(observed != (fitted>cutoff))/K 
    ### in cv.glm the actual error is calculated as (y - p(y=1)) 
    # error <- error + mean((observed - fitted)^2)/K 
    ### the mis-classification rate will depend on how we decide what is assigned to each category 
  }
  error
}


make_conf_mat <- function(predicted, actual) {
  table(predicted = predicted, actual = actual)
}

get_sens <- function(conf_mat) {
  conf_mat[2, 2] / sum(conf_mat[, 2])
}
# Note that this function is good for illustrative purposes, but is easily broken. (Think about what happens if there are no "positives" predicted.)


get_spec <-  function(conf_mat) {
  conf_mat[1, 1] / sum(conf_mat[, 1])
}
```

##


