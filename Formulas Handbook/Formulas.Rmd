# Formulas Handbook

To better

## Setting Up and Basic commands

Get and Set working directory.
```{r, eval=FALSE}
# getwd()
# setwd()
```

Import dataset
```{r}
# mrents <- read.table("rent99.raw", header = TRUE)
pengs <- read.csv("C:/Users/PayThePizzo/Projects/PredictiveAnalysisNotes/Labs/lab01/penguins.csv")
```


Show a summary of the data

* Returns `Min.`, `1st Qu.`, `Median`, `Mean`, `3rd Qu.`, `Max`, `NA's` for numerical variables
* Returns `Length`, `Class`, `Mode` for categorical variables

```{r, eval=FALSE}
summary(pengs)
```
Remove all missing values

```{r}
pengs <- na.omit(pengs)
```

Define 


```{r}

```

---

## Plots

Quick scatterplot
```{r}
scatterplot <- function(dataset, x, y){
    plot(dataset[, c(x,y)])
}

scatterplot(pengs, "flipper_length_mm", "body_mass_g")
```

Histogram
```{r}
#hist(mrents$rent, xlab = "Rent")
#abline(v=mean(mrents$rent), col = 2, lwd =2)
#abline(v=median(mrents$rent), col = 4, lwd =2)
#legend("topright", bty = "n", col = c(2,4),lwd = 2, legend = c("mean","median"))
```

Barplot
```{r}
#par(mfrow=c(1,2))
#barplot(table(mrents$kitchen)) 
#barplot(table(mrents$cheating))
```

Boxplot

```{r}
#par(mfrow=c(1,2))
#boxplot(rent~kitchen, data = mrents)
#boxplot(rent~bath, data = mrents)
```

```{r}
#mrents$kitchen_level <- factor(mrents$kitchen, labels = c("Standard","Premium"))
#boxplot(rent~kitchen_level, data = mrents, col = c(rgb(0.45,0,0,0.4),rgb(0.118,0.565,1,0.4)),
#        border= c("darkred","dodgerblue"))
```

Plot
```{r}
#plot(rent~area, data = mrents, pch =16, bty = "l",col="darkorange2")
#title(main = "relazione tra area e affitto")
```

---

## Some Statistics

```{r}

```

---

## SLR - Least Squares

$$Y = \beta_{0} + \beta_{1}X + \epsilon$$

```{r, eval=FALSE}
fit <- lm(formula = target ~ predictor, data = dataset)
```

### $\hat{\beta}$ parameters estimates

$$\hat{\beta}_1 = \frac{c_{XY}}{s^2_{X}} =  r_{XY} \frac{s_{Y}}{s_{X}},\quad \mbox{and} \quad \hat{\beta}_0 = \overline{y}-\hat{\beta}_1 \overline{x}$$

```{r, eval=False}

```


## SLR - Gaussian Noise

$$$$


```{r, eval=FALSE}
fit <- lm(formula = target ~ predictor, data = dataset)
```



```{r}
#x <- pengs$body_mass_g
#y <- pengs$flipper_length_mm
#n <- length(x)

# <- sum((x-mean(x))^2)/n
#s2y <- sum((y-mean(y))^2)/n

#covxy <- cov(x,y) 
#rxy <- cor(x,y)

#mx <- mean(x)
#my<- mean(y)

# Angular coefficient
# Dictates how the y changes in proportion to x
#beta1_hat <- rxy * sqrt(s2y/s2x)
# Intercept
# Forces the regression to pass by the sample mean of x and y.
#beta0_hat <- my - beta1_hat *mx

# The coefficients
#c(beta0_hat, beta1_hat)
```

```{r}
computeSumSquares <- function(bs, y, x){
  observed <- y
  modelled <- bs[1] + bs[2] * x
  squares <- (observed - modelled)^2
  sum(squares)
}
```

###

### Inference

#### Confidence Intervals

#### Prediction Intervals

#### Hypothesis Testing


---

## MLR

$$$$

```{r, eval=FALSE}
##lm - modelli lineari  -------
df <- data.frame(x1 = runif(15), x2 = runif(15), y  = rnorm(15))

## stima del modello 
fit <- lm(y~x1+x2, data = df)
summary(fit) ## varie informazioni riassuntive sulla stima
coef(fit) ## valori stimati dei coefficienti del modello 
confint(fit) ## intervalli di confidenza per i coefficienti del modello 

## per aggiungere trasformazioni di X come predittori si usa la funzione I(.)
## fit <- lm(y~x1+x2+I(x2^2), data = df) 
## per polinomi 
## fit <- lm(y~x1+poly(x2,2), data = df)
## fit <- lm(y~x1+poly(x2,2, raw=TRUE), data = df)

## la matrice di disegno usata nella stima
model.matrix(fit)

## predizione 
# per i valori osservati delle X
fitted(fit)
predict(fit) ## predict produce anche intervalli di confidenza e predizione
predict(fit, interval = "confidence") 
predict(fit, interval = "prediction") 
## per un nuovo set di punti 
nd <- data.frame(x1 = c(0.2,0.8), x2 = c(0.3,0.6))
predict(fit, newdata = nd)

# residui 
residuals(fit) ## these are y - fitted(fit)
rstandard(fit) ## standardised residuals 
rstudent(fit)  ## studentized residuals 

# goodness of fit/ bontC  di adattamento 
plot(fit) # grafici riassuntivi
AIC(fit, k = 2); BIC(fit); logLik(fit) ## verosimiglianza e criteri di informazione
hatvalues(fit) ##  leverages - punti di leva 
car::vif(fit) ## variance inflation factors - ci sono problemi di colinearitC ? 
cooks.distance(fit) # outliers/punti particolari 

## test anova per modelli annidati 
# anova(small_model, big_model)
anova(lm(y~x1, data = df), fit)


## model selection 
## la funzione step qui usata per un algoritmo forward come esempio 
## opzioni importanti 
# scope per delineare l'ambito della ricerca
# k: per definire la penalizzazione del criterio di informazione
# direction: per la direzione: forward, backward, both

step(object = lm(y~1, data = df), 
     scope = list(lower = lm(y~1, data = df), 
                  upper = fit), 
     direction = "forward",
     k = 2
) ## 


## trasformazione di Box-Cox 
## da usare se y|X risulta non-normale 
## MASS::boxcox

```

---

## GLM

$$$$

```{r, eval=FALSE}
##glm - modelli lineari generalizzati -------

##in questo esempio usiamo una Poisson 

df <- data.frame(x1 = runif(15), x2 = runif(15), y  = rpois(15, 6))

## stima del modello 
fit <- glm(y~x1+x2, data = df, family = poisson()) 
## di default si usa la funzione legame canonica
poisson()$link
summary(fit) ## varie informazioni riassuntive sulla stima
coef(fit) ## valori stimati dei coefficienti del modello 
confint.default(fit) ## intervalli di confidenza per i coefficienti del modello 

## predizione 
# per i valori osservati delle X
fitted(fit) ## predizione sulla scale di Y (exp(linear.predictor))
predict(fit) ## predict di default mostra il predittore lineare
predict(fit, type = "response") ## predict accetta un'opzione type per mostrare i valori stimati sulla scala delle Y 
## per un oggetto glm predict non puC2 costruire intervalli di confidenza (e non si possono costruire intervalli di predizione)
predict(fit, se.fit = TRUE) # con opzione se.fit si ottiene lo standard error per il predittore lineare 
## per un nuovo set di punti 
nd <- data.frame(x1 = c(0.2,0.8), x2 = c(0.3,0.6))
a <- predict(fit, newdata = nd, se.fit = TRUE); a
# intervalli di confidenza manuali
alpha = 0.05
cbind(a$fit + qnorm(alpha/2) * a$se.fit, 
      a$fit + qnorm(1-alpha/2) * a$se.fit)
      
        
# residui 
residuals(fit) ## di default deviance residuals 
residuals(fit, type = "pearson") ## type = c("deviance", "pearson", "response"))

# goodness of fit/ bontC  di adattamento 
plot(fit) # grafici riassuntivi
AIC(fit, k = 2); BIC(fit); logLik(fit) ## verosimiglianza e criteri di informazione

## test anova per modelli annidati 
# anova(small_model, big_model)
anova(glm(y~x1, data = df, family=poisson()), fit, test = "LRT")



## glm as a classifier ------------- 

## funzioni implementate nelle slides/laboratorio 


cv_class <- function(K=5, dat, model, cutoff = 0.5){
  assign_group <- rep(seq(1,K), each = floor(nrow(dat)/K))
  ### this ensures we use all points in the dataset
  ### this way we might have subgroups of different size 
  if(length(assign_group) != nrow(dat)) assign_group <- c(assign_group, sample(seq(1,K)))[1:nrow(dat)] 
  assign_group <- sample(assign_group, size = nrow(dat))
  error <- 0
  for(j in 1:K){
    whichobs <- (assign_group == j)
    ## fit a model WITHOUT the hold-out data
    folded_model <- suppressWarnings(glm(model$formula, 
                                         data = dat[!whichobs,], 
                                         family = "binomial"))
    ## evaluate the model on the hold-out data
    fitted <- suppressWarnings(predict(folded_model,
                                       dat[whichobs,], 
                                       type="response"))
    observed <- dat[whichobs, strsplit(paste(model$formula), "~")[[2]]]
    error <- error + mean(observed != (fitted>cutoff))/K 
    ### in cv.glm the actual error is calculated as (y - p(y=1)) 
    # error <- error + mean((observed - fitted)^2)/K 
    ### the mis-classification rate will depend on how we decide what is assigned to each category 
  }
  error
}


make_conf_mat <- function(predicted, actual) {
  table(predicted = predicted, actual = actual)
}

get_sens <- function(conf_mat) {
  conf_mat[2, 2] / sum(conf_mat[, 2])
}
# Note that this function is good for illustrative purposes, but is easily broken. (Think about what happens if there are no "positives" predicted.)


get_spec <-  function(conf_mat) {
  conf_mat[1, 1] / sum(conf_mat[, 1])
}
```

---

## GLM - Classifier


