---
title: "Lab 4 - Model selection"
author: "Ilaria Prosdocimi"
output: html_document
---
  
Prendiamo in esame il dataset `prostate`: 
  
```{r}
urlLocation <- "https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data"
prostate <- read.table(urlLocation, header=TRUE)[,1:9]
## explanation of the data 
## https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.info.txt
# write.csv("../data/prostate.csv")
```

Le variabili nel dataset sono: 

* lcavol: log cancer volume
* lweight: log prostate weight
* age: age of patient
* lbph: log of the amount of benign prostatic hyperplasia
* svi: seminal vesicle invasion (binary variable)
* lcp: log of capsular penetration
* gleason: Gleason score
* pgg45: percent of Gleason scores 4 or 5
* lpsa: log PSA score


Desideriamo costruire un modello che abbia `lpsa` come variabile risposta e una o più delle altre variabili come variabili esplicative:  questo può essere utile sia per uno scopo predittivo (cioè stimare il valore di `lpsa` per un nuovo paziente) che per scopo inferenziale (cioè capire quali siano le variabili che  hanno un effetto sul valore di `lpsa`).  

Il grafico mostra la relazione di `lpsa` con tutte le altre variabili nel dataset (NB: questo tipo di analisi esplorativa è possibile solo se ci sono poche variabili nel dataset): 

```{r plotMatrix}
plot(prostate)
```

# Model selection tramite algoritmi step-wise 

I due estremi a cui si può pensare sono un modello che non includa nessun predittore  (cioè un modello con solo l'intercetta) e un modello che includa tutti i predittori. Per poter fare la stima in maniera corretta dobbiamo trasformare la variabile `svi` da numerica ad una variabile `factor`, cioè una variabile categorica. 


```{r}
prostate$svi <- as.factor(prostate$svi)
fit_int_only <- lm(lpsa~1,data = prostate)
fit_all_vars <- lm(lpsa~.,data = prostate)
```

La prima domanda che possiamo porci è se il modello più complesso sia _significativo_


```{r}
anova(fit_int_only, fit_all_vars)
```

Includere tutti i predittori sembra spiegare una buona parte della variabilità dei dati. 
Forse però esiste un modello che è una via di mezzo tra i due estremi e spiga una proporzione della variabilità dei dati comparabile con quella spiegata da `fit_all_vars` ma usando meno parametri. Per fare questa valutazione possiamo usare AIC, in cui  la bontà di adattamento (misurata dalla verosimiglianza del modelli) viene penalizzata per il numero di gradi di libertà usati dal modello. Il valore di AIC per il modello con la sola intercetta è `r AIC(fit_int_only)`. Potremmo cercare di trovare il modello con un solo predittore che porta ad il più grande miglioramene in termini di AIC (cioè il valore di AIC minore):


```{r}
for(j in 1:(ncol(prostate)-1)) print(c(names(prostate)[j],AIC(lm(prostate$lpsa~prostate[,j]))))
## the lcavol variable is the one that reduces AIC the most 
```

Il passo successivo sarebbe quello di controllare se esiste un modello che include due variabili esplicative per cui si riesce ad ottenere un valore di AIC ancora più piccolo. Invece che fare questo procedura manualmente usiamo la funzione `step` per eseguire un algoritmo di ricerca del modello in _forward search_, cioè una ricerca che parte da un modello piccolo e va via via ad aumentare la complessità del modello fino a che non è più possibile migliorare il valore di AIC aggiungendo predittori: 

```{r}
fit_int_only <- lm(lpsa~1,data = prostate)
fit_all_vars <- lm(lpsa~.,data = prostate)
sel_forw <- step(object = fit_int_only, direction = "forward", 
                 ## scope give the lower, i.e. simplest, and upper, most complex, limits 
                 ## the "range" of models in which the algorithm will perform its search 
     scope = list(lower=fit_int_only, upper = fit_all_vars))
```

Dopo 5 iterazioni l'algoritmo si ferma e identifica un modello che include le variabili: 

```{r}
names(sel_forw$coefficients)
```

Aggiungere un altra variabile esplicativa non porta a miglioramenti del valore di AIC. Notiamo immediatamente che il valore di AIC per il modello individuato (`r AIC(sel_forw)`) è di molto minore al valore di AIC del modello con la sola intercetta. 


AIC viene calcolato con: 
\[AIC(M) = - 2 * logLik(M) + 2 * p(M)\]
dove $p(M)$ è il numero di gradi di libertà del modello $M$. 

Per i modelli lineari, questo è equivalente (a meno di una costante): 
\[AIC(M) = \text{constant} + n*MSS(M) + 2 * p(M)\]

In R questo si può derivare con: 


```{r}
-2*as.numeric(logLik(sel_forw)) + 2*(length(sel_forw$coefficients)+1)
## or more simply
AIC(sel_forw)
### step uses the MSS based definition 
nrow(sel_forw$model)*log(mean(sel_forw$residuals^2)) + 2*(length(sel_forw$coefficients))
## it's OK to use slightly different definition up or minus a constant, since what matters is the ranking 
```

Un altro possibile approccio è quello di usare un algoritmo di _backward search_, in cui si parte da un modello complesso e si va via via a togliere un variabile alla volta fino a che si continua a diminuire il valore di AIC: 

```{r}
sel_back <- step(object = fit_all_vars, direction = "backward", 
     scope = list(lower=fit_int_only, upper = fit_all_vars))
```

Per questo dataset i due algoritmi trovano lo stesso modello (non sempre è così). 

L'ultimo approccio possibile è l'approccio _stepwise_ in cui ad ogni iterazione l'algoritmo verifica se aggiungere o togliere una variabile migliora il valore di AIC. L'algoritmo può essere inizializzato con modelli molto semplici, complessi, o intermedi: 

```{r}
sel_both <- step(object = fit_int_only, direction = "both", 
     scope = list(lower=fit_int_only, upper = fit_all_vars))
sel_both_bis <- step(object = fit_all_vars, direction = "both", 
     scope = list(lower=fit_int_only, upper = fit_all_vars))
### regardless of where we start we find the same model
identical(sort(names(sel_both$coefficients)), 
          sort(names(sel_both_bis$coefficients)))
sel_both_intermediate <- step(object = lm(lpsa ~ lcavol + gleason, data = prostate),
                              direction = "both", 
     scope = list(lower=fit_int_only, upper = fit_all_vars))
```

In questo esempio tutti e tre gli approcci portano alla scelta dello stesso modello: 

```{r}
identical(sort(names(sel_back$coefficients)), 
          sort(names(sel_forw$coefficients)))
identical(sort(names(sel_back$coefficients)), 
          sort(names(sel_both_bis$coefficients)))
```

Possiamo ora controllare come il modello si adatta ai dati: 

```{r}
summary(sel_back)
plot(fitted(sel_back),prostate$lpsa, pch=16)
abline(0,1) ## not bad 
```

Il modello è significativo contro il modello nullo e spiega una buona proporzione dei dati. Quando invece confrontiamo il modello selezionato con il modello più complesso non vediamo una differenza significativa nel valore di variabilità spiegato: 

```{r}
anova(sel_back, fit_all_vars)
```

Fino ad ora abbiamo usato il valore di AIC per decidere quali variabili includere nel modello. Possiamo anche usare il valore di BIC, in cui la penalizzazione della complessità del modello è più forte (specie quando $n$ è grande) dato che la sua forma è: 

\[BIC(M) = - logLik(M) + \log(n) * p(M).\]

Confrontiamo AIC e BIC per i modelli ai due estremi della complessità: 

```{r}
AIC(fit_all_vars); AIC(fit_int_only)
BIC(fit_all_vars); BIC(fit_int_only)
## alternatively AIC(fit_int_only, k=log(nrow(prostate)))
```

Possiamo ancora usare la funzione step `step` per fare model selection cercando modelli che minimizzino il valore di BIC: 

```{r}
sel_bic_back <- step(object = fit_all_vars, direction = "backward", 
     scope = list(lower=fit_int_only, upper = fit_all_vars),k=log(nrow(prostate)))
sel_bic_forw <- step(object = fit_int_only, direction = "forward", 
     scope = list(lower=fit_int_only, upper = fit_all_vars),k=log(nrow(prostate)))
sel_bic_both <- step(object = fit_all_vars, direction = "both", 
     scope = list(lower=fit_int_only, upper = fit_all_vars),k=log(nrow(prostate)))
```


# Validazione incrociata: Leave-one-out cross validation

Un approccio molto diverso alla model selection è quello di usare la validazione incrociata o leave-one-out cross-validation. L'idea è quella di valutare i modelli considerando la loro variabilità quando usati per predire nuove osservazioni (modelli troppo complessi e sovra-parametrizzati tenderanno ad essere poco generalizzabili e a fare predizioni molto variabili). Quindi invece che usare solo misure di bontà di adattamento _in-sample_ (in cui usiamo lo stesso campione per stimare e valutare il modello) si definisce una quantità che misuri la capacità del modello di essere affidabile _out-of-sample_ usando parti diverse del campione per stimare il modello e valutarne la bontà di adattamento. In particolare si quantifica l'errore che possiamo aspettarci di fare  quando si usa un modello stimato per predire una nuova osservazione calcolando il leave-one-out error $e[i] = y_i - \hat{y}[i]$, dove $\hat{y}[i]$ indica il valore per l'osservazione $i$ ottenuta con un modelo stimato senza usare l'osservazione $i$. 

Esemplifichiamo la cosa usando il modello `sel_bic_both`: 

```{r}
i = 1 ; X <- model.matrix(sel_bic_both)
esqi <- prostate$lpsa[i] -  X[i,]%*%lm(lpsa ~ lcavol + lweight + svi, data = prostate[-i,])$coef
esqi
```

Il valore di $RMSE_{LOOCV}$ viene calcolato con: 

```{r}
esq <- rep(NA,length = nrow(prostate))
for(i in seq_along(esq)){
  esq[i] <- prostate$lpsa[i] -  X[i,]%*%lm(lpsa ~ lcavol + lweight + svi, data = prostate[-i,])$coef
}
sqrt(mean(esq^2))
```

Per i modelli lineari tuttavia, si può dimostrare che il valore di $RMSE_{LOOCV}$ può essere derivato senza dover stimare effettivX amente il modello $n$ volte usando la seguente formula:

\[
\text{RMSE}_{\text{LOOCV}} = \sqrt{\frac{1}{n}\sum_{i=1}^n \left(\frac{e_{i}}{1-h_{ii}}\right)^2},
\]

dove $h_{ii}$ sono gli elementi diagonali della matrice cappello (hat matrix): 

```{r}
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
calc_loocv_rmse(sel_bic_both)
```

Da dove vengono gli `hatvalues`? 

```{r}
head(hatvalues(sel_bic_both))
H <- X %*% solve(crossprod(X)) %*% t(X)
head(diag(H))
```

Ora possiamo quindi usare la funzione `calc_loocv_rmse` per confrontare alcuni dei modelli che abbiamo usato fino ad ora: 

```{r}
calc_loocv_rmse(sel_back)
calc_loocv_rmse(sel_bic_back)
calc_loocv_rmse(fit_all_vars)
```

`sel_bic_back` è il modello con il valore di $RMSE_{LOOCV}$ più basso. 


# Verifica delle assunzioni del modello

Abbiamo individuato alcuni modelli ottimali: a seconda dell'approccio usato abbiamo identificato modelli diversi. Questo mostra come sia difficile identificare un modello "giusto" (spesso poi non esiste un modello vero, ma modelli competitivi tra loro). 
Tuttavia, per qualunque modello che possa essere stato identificato come ottimale da qualche criterio è poi necessario valutare la validità delle assunzioni sotto a cui sono derivate le stime del modello stesso (e su cui si fondano le quantità usate per valutare la bontà di adattamento). 
Queste assunzioni sono: 

* Linearità (della relazione tra le X e la variabile risposta); 
* Indipendenza (delle osservazioni/errori tra loro); 
* Normalità degli errori; 
* Eguaglianza delle varianze (Omoschedasticità)
 

Verifichiamo queste assunzioni per il modello `sel_bic_both`.  

* Linearità ed eguaglianza delle varianze 

La relazione tra i predittori e la variabile risposta è lineare? O, per dirla in altro modo, dopo che è stata stimata la relazione lineare tra il predittore e la variabile risposta, resta della relazione con una forma strutturata tra il predittore e i residui? E la forma tra i valori stimati e i residui è una variabile casuale o rimane una forma che indica che non abbiamo catturato qualche relazione strutturale nei dati? 

Inizzimao con un grafico di ciascun predittore contro i residui: 

```{r}
sel_model <- sel_bic_both
whichCols <- names(sel_bic_both$coefficients)[-1]
if(any(whichCols == "svi1")) whichCols[whichCols == "svi1"] <- "svi"
par(mfrow=c(2,ceiling(length(whichCols)/2)),pch=16)
for(j in seq_along(whichCols)){
 plot(prostate[,whichCols[j]], residuals(sel_model)) 
  abline(h=0,lty=2)
}
```

A seguire il grafico dei valori stimati contro i residui: 

```{r fittedAginstResid}
plot(sel_model$fitted.values, sel_model$residuals)
abline(h=0,lty=2)
# can also be derived with
# plot(sel_model,which=1)
```

Questi grafici permettono anche di identificare possibili problemi con l'assunzione di omoschedasticità (varianze uguali). 

Per il modello `sel_bic_both` non identifichiamo problemi rilevanti.

* Normalità 

Ora controlliamo la normalità tramite un qqplot: 

```{r}
qqnorm(sel_model$residuals)
qqline(sel_model$residuals)
# can also be derived with
# plot(sel_model,which=2)
```

Il qqplot non mostra deviazioni di rilievo.

Valutare l'indipendenza delle osservazioni (e meglio, degli errori) è una cosa che è facile fare con test o grafici. Possiamo considerare lo schema di campionamento e valutare se è credibile che le osservazioni siano indipendenti tra loro. Nel nostro dataset le osservazioni sono pazienti che hanno subito un intervento in un ospedale, e quindi non c'è una ragione evidente per cui si possa pensare che le misurazioni sui diversi pazienti siano dipendenti. Non sappiamo però ad esempio se alcuni dei pazienti sono legati da legami di parentela (e possano quindi avere delle caratteristiche comuni) o se siano parte di una popolazione esposta a qualche particolare forma di inquinamento che potrebbe portare a relazioni tra le variabili diverse da quelle della popolazione generale. Se invece i pazienti fossero stati osservati in ospedali diversi potremmo anche dover controllare se i pazienti nello stesso ospedale sono più simili tra loro dato che sono valutati dallo stesso personale medico usando le stesse strumentazioni. Dato che non abbiamo tutte queste informazioni teniamo per buona l'assunzione di indipendenza tra le osservazioni. 

\ 
 
## The qqplot

Il qqplot permette di confrontare la distribuzione dei residui contro quella di una normale. Cosa mostra esattamente il grafico? L'idea di base è di confrontare il campione osservato (ordinato) contro il campione teorico di dimensione $n$ che potremmo aspettarci di estrarre  da una distribuzione normale. Che forma ha questo campione teorico/ideale da una normale? Possiamo pensare di estrarre il valore dei quantili di una normale legate alle probabilità $(1,\ldots,n)/(n+1)$, cioè derivare i quantili legati a probabilità distribuite uniformemente tra 0 e 1: 

```{r}
## empirical quantiles 
n <- nrow(sel_model$model)
head(sort(sel_model$residuals))
## theoretical quantiles under the standard normal
head(qnorm(seq(1,n)/(n+1)))
plot(qnorm(seq(1,n)/(n+1)),
     sort(sel_model$residuals))
```


Come Ci sono diversi modi di ottenere la divisione dello spazio tra 0 e 1 in parti uguali e questo può influenzare l'aspetto del qqplot (in particolare quando la dimensionalità campionaria è piccola). La formula per dividere lo spazio della probabilità si scrive in maniera generale con 
\[ \frac{i-a}{n+1-2a}\]
individuando le cosiddette _plotting positions_ - in `?ppoints` sono dati alcuni dettagli su come R implementa questo calcolo. Di default se $n>10$ si ha $a=0.5$, e quindi: 

\[\frac{i-0.5}{n}\]

```{r}
plot(qnorm((seq(1,n)-0.5)/n),
     sort(sel_model$residuals))
```


