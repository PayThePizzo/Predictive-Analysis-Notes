---
title: "Lab 1 - Regressione Lineare in R"
author: "Ilaria Prosdocimi"
date: "Semestre 1 - AA 2022/23"
output:
  html_document:
    fig_caption: yes
    theme: flatly
    highlight: pygments
    code_folding: show
    toc: yes
    toc_depth: 2
    number_sections: yes
    toc_float:
      smooth_scroll: no
  pdf_document:
    toc: yes
    toc_depth: '2'
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Setting working directory if needed
```{r}
# Set working directory
setwd("C:/Users/PayThePizzo/Projects/PredictiveAnalysisNotes/Labs/lab01")
# Get working directory
getwd()
```

# Dataset Loading and Cleaning

In this lab we will use a file containing information regarding the Palmers Islands' penguins

```{r}
pengs <- read.csv("../data/penguins.csv")
summary(pengs)
```

By applying `summary()` to the dataset, R returns:

1. For numeric columns
    + Min, Mean, Max
    + 1st, Median, 3rd quartiles
    + NA's, the number of missing values (rows with no data or non-numeric data)
2. For categorical columns
    + Length
    + Class
    + Mode

Here we perform some operations to catpure a subset of the dataset where the 
information is complete. In this case we remove all missing values, even though
it not always the best option!

```{r}
pengs <- na.omit(pengs)

# Check the number of rows after cleaning 
nrow(pengs)
```

---

# SLR - Manual mode

For our model: $Y = \beta_{0} + \beta_{1}X + \varepsilon$, we consider as our target `flipper_length_mm` which varies in function of `body_mass_g` of a penguin. 

```{r}
plot(pengs[,c("flipper_length_mm","body_mass_g")])
```

Through the plug-in principle we can substitute the coefficients with their estimates: 

\[\hat{\beta}_1 = \frac{c_{XY}}{s^2_{X}} =  r_{XY} \frac{s_{Y}}{s_{X}},\quad \mbox{and} \quad \hat{\beta}_0 = \overline{y}-\hat{\beta}_1 \overline{x}  \]

```{r byHandCalc}
x <- pengs$body_mass_g
y <- pengs$flipper_length_mm
n <- length(x)

s2x <- sum((x-mean(x))^2)/n
s2y <- sum((y-mean(y))^2)/n

covxy <- cov(x,y) 
rxy <- cor(x,y)

mx <- mean(x)
my<- mean(y)

# Angular coefficient
# Dictates how the y changes in proportion to x
beta1_hat <- rxy * sqrt(s2y/s2x)
# Intercept
# Forces the regression to pass by the sample mean of x and y.
beta0_hat <- my - beta1_hat *mx

# The coefficients
c(beta0_hat, beta1_hat)
```

We can visualize our results through a plot now

```{r}
# Plot 2D for (xi,yi)
# plot(target ~ predictor, data = df)
plot(flipper_length_mm ~ body_mass_g, data = pengs,pch = 16)
# Adds line y=bo+b1x
abline(beta0_hat, beta1_hat, col = 2, lwd = 1.4)
```

Because of the assumptions made, we are sure the $\hat{Y}$ is the function that minimizes the MSE and that no other combination of coefficients can lower that value. We can confirm by comparing our results with any combination of values for $\beta_{0}$ and $\beta_{1}$

```{r}
# Estimated values
yhat <- beta0_hat +  beta1_hat * x  

# Empirical MSE 
(sum((y-yhat)^2))
# Some random coefficients
(sum((y-(137+0.016*x))^2))
```

Alternatively we can use the linear models defined in R.

---

# SLR in R 

The main function to estimate linear models (with the least squares method) in R is `lm()`

The first variable of `lm` is `formula = target ~ predictor1 + predictor2 + ...` where we could have one or more predictors. 

It is important to specify the dataset each time through `data = dataset`

```{r}
fit <- lm(flipper_length_mm~body_mass_g, data = pengs)
```

We can *print* `fit` to find some basic insights: 

```{r}
fit ## in R this corresponds to print(fit)
```

This prints the code of the model built and its coefficients (estimated through the least squares method).

We can access them through:

```{r}
coef(fit)
```

## lm class

To `lm` objects we can apply different functions: 

```{r}
class(fit)
```

One the most important function is `fitted`,which derives the estimated values for $y_i$ (namely evaluating the function $\beta_0 + \beta_1 x_i$ and taking the least squared estimates for $\beta_0$ and $\beta_1$): 

```{r}
# From lm
(head(fitted(fit),10))
# Our estimates
(head(yhat,10))
```

We want to get the residuals of the model through
```{r}
# From lm
(head(residuals(fit),10))

# Our residuals
(head(y - yhat,10))
```

And the standard error $s_{e}$, which allows to get info about the variances of the beta-parameters
```{r}
# For lm
(se <- sqrt (sum((residuals(fit))^2) / (n-2)))

# Our se
(se <- sqrt (sum((yhat - y)^2) / (n-2)))
```

---

# `summary()` function

A very useful function is `summary` which simplifies all things we just showed.

```{r}
summary(fit)
```

```{r}
sse <- sum((pengs$flipper_length_mm - fit$fitted.values)^2)

s2e <- (n/(n-2))*(sse/n)
se <- sqrt(s2e)

altse <- sqrt((sse/(n-2)))

```

Which prints out:

1. Call, the model
    + We can see the model built through `summary(fit)$call`
2. Residuals, the errors of the model 
    + They should be homoskedastic and have sample variance equal to 0
    + If the distribution was symmetric (which we hope it is), the mean and the median should coincide, the `min` and `max` values should be specular, and the `1Q` and `3Q` should too.
    + We can get them through `summary(fit)$residuals`
3. Coefficients, the beta parameters' values
    + also available through `summary(fit)$coefficients`
    + their Std. Error, `summary(fit)$sigma`
    + their t-value,
    + their p-value,
4. Residual Standard Error, the estimated variance of the errors $s_{e}$ which we can get through `summary(fit)$sigma`
    + with `n-p` degrees of freedom, where `p` is the count of estimated $\beta$ parameters and `n` is the count of the rows (instances). We can even get the residual degrees of freedom for the model through `fit$df.residual`
5. Multiple R-Squared, $R^{2}$ a measure of the model's goodness of fit
    + `summary(fit)$r.squared`
6. Adjusted R-Squared
    + `summary(fit)$adj.r.squared`
7. F-statistic
    + `summary(fit)$fstatistic`
8. P-value


We can find some utils we can call to get our data from 
```{r}
names(summary(fit))
```

# SLR - Uncertainty of the model

We can see that the estimated value of $\sigma^2$, or better $s_{e}$ is present in `Residual Standard Error`, which is the variance of random variable describing the error of the model, $\varepsilon$: 

\[s_e^2 = \frac{1}{n}\sum_{i=1}^{n-2} (y_i - \hat{y}_i)\]

```{r}
# se is Residual Standard Error
summary(fit)$sigma
(se <- sqrt(sum(residuals(fit)^2)/(n-2)))
(se <-sqrt(sum((pengs$flipper_length_mm  - fitted(fit))^2)/(nrow(pengs)-2)))
```

$\hat{\sigma}$ is used to estimate the uncertainty of the model's coefficient estimates (the variances of the $\beta$ parameters):
* $Var[\widehat{\beta}_0]$ = `Std. Error` of the $\widehat{\beta}_0$
* $Var[\widehat{\beta}_1]$ = `Std. Error` of the $\widehat{\beta}_1$


$$Var[\widehat{\beta}_0]= sigma_e^2\left[\frac{1}{n} + \frac{\overline{x}^2}{n s^2_X}\right]$$
$$Var[\widehat{\beta}_1] = \frac{ sigma_e^2}{n s^2_X}$$

```{r}
n <- nrow(pengs)

s2x <- sum((pengs$body_mass_g - mean(pengs$body_mass_g))^2)/n

# Alternative way to get coefficients
c(se * sqrt(1/n + mean(pengs$body_mass_g)^2/(n * s2x)), se * sqrt(1/(n * s2x)))

summary(fit)$coef[,2]
```

---

**Esercizio**

Cosa succede alla variabilità della stima dei coefficienti se si usa come variabile esplicativa una nuova variabile 
```{r}
pengs$mass_minus_4000 <- pengs$body_mass_g - 4000
```

E cosa succede invece se il peso viene espresso in grammi: 
```{r}
pengs$body_mass_Kg <- pengs$body_mass_g/1000
```

Infine, cosa cambia invece quando si cambia anche la variabile risposta, per esempio esprimendo il valori in cm
```{r}
pengs$bill_length_cm <- pengs$bill_length_mm*10
```


Anche il valore di $R^2$ è presente nell'output:

```{r}
summary(fit)$r.squared

## SSreg/SStot
sum((fitted(fit) - mean(pengs$flipper_length_mm))^2)/sum((pengs$flipper_length_mm  - mean(pengs$flipper_length_mm ))^2)

# 1- SSres/SStot 
1 - sum((pengs$flipper_length_mm  - fitted(fit))^2)/sum((pengs$flipper_length_mm  - mean(pengs$flipper_length_mm ))^2)
```

Infine la funzione `residuals` restituisce i residui del modello: 

```{r}
head(residuals(fit))
head((pengs$flipper_length_mm - fitted(fit)))
```

Ricordiamo alcune proprietà dei residui che derivano dalle equazioni di stima (estimating equations): 

```{r}
# mean of residuals is null 
mean(residuals(fit))
# no correlation with X - 
# this does not mean that there is not relationship left between X and the residuals 
cor(residuals(fit), pengs$body_mass_g)
```


# Predizione ed incertezza

Ora che il modello è stato stimato, possiamo usarlo per predirre nuovi valori di $Y|X=x$: mentre la funzione `fitted` restituisce i valori di $Y$ stimati *per le x osservate nel campione* ($\hat{y_i}$), possiamo usare la funzione `predict` per valutare la funzione in dei diversi valori di $x$ (di default però la funzione predice i valori nel campione osservato):  

```{r fittedValues,class.source = "fold-show"}
head(fitted(fit))
head(predict(fit))
predict(fit, newdata = data.frame(body_mass_g = c(3500,4000,4700)))
coef(fit)[1]+coef(fit)[2] * 3500; coef(fit)[1]+coef(fit)[2] * 4000; coef(fit)[1]+coef(fit)[2] * 4700
## what does the slope value mean? 
predict(fit, newdata = data.frame(body_mass_g = 3501)) - predict(fit, newdata = data.frame(body_mass_g = 3500))
coef(fit)[2]
## what does the intercept value mean? 
predict(fit, newdata = data.frame(body_mass_g = 0))
coef(fit)[1]
```


La funzione `predict` inoltre permette di specificare se desideriamo ottenere qualche informazione sull'incertezza della stima: in particolare si possono ottenere i valori della deviazione standard attorno ad $E[m(x)]$, derivati come:
\[\hat{SE}[\hat{m}(x)] = s_e\sqrt{\frac{1}{n}+\frac{(x-\bar{x})^2}{\sum_{i = 1}^n(x_i-\overline{x})^2}}\]
dove $s_e$ è la deviazione standard dell'errore:
```{r, class.source = "fold-show"}
sepengs <- sqrt(sum((pengs$flipper_length_mm-fit$fitted.values)^2)/(nrow(pengs)-2))
sepengs
```

Il valore della deviazione standard attorno al valore atteso di $Y|X=x$ dipende da $x$, il valore in cui valutiamo la funzione $m(x)$: 

```{r}
n <- nrow(pengs)
# error for x = 4000
sepengs * sqrt((1/n + ((4000-mean(pengs$body_mass_g))^2)/sum((pengs$body_mass_g-mean(pengs$body_mass_g))^2)))
(pred1 <- predict(fit, newdata = data.frame(body_mass_g = 4000), se.fit = TRUE))
pred1$se.fit
# error for x = 5000
sepengs * sqrt((1/n + ((5000-mean(pengs$body_mass_g))^2)/sum((pengs$body_mass_g-mean(pengs$body_mass_g))^2)))
pred2 <- predict(fit, newdata = data.frame(body_mass_g = 5000), se.fit = TRUE)
pred2$se.fit
```


Possiamo derivare l'incertezza attorno a diverse predizioni: 

```{r}
mean(pengs$body_mass_g)
predict(fit, newdata = data.frame(body_mass_g = c(1500,4200,5000,8000)), se.fit = TRUE)$se.fit
```

Si nota come vi sia incertezza sempre maggiore più il valore di $x$ è distante da $\overline{x}$. 

# Verifica delle assunzioni del modello 

La stima dei minimi quadrati si basa su alcune assunzioni non troppo stringenti: si assume che la relazione tra $X$ ed $Y$ possa essere approssimata da una relazione lineare. Altre assunzioni sono necessarie per poter derivare proprietà essenziali degli stimatori, in particolare si assume che gli errori del modello siano indipendenti e identicamente distribuiti con varianza costante. 

Sebbene non sia possibile osservare gli errori del modello, possiamo osservare i residui, cioè la differenza tra valori osservati e stimati (dal modello) della variabile risposta: $r_i = (y_i-\hat{y}_i)$. In R si possono ottenere i valori di $r_i$ con `residuals`: 

```{r residuals, class.source = "fold-show"}
head(residuals(fit))
# by hand
head(pengs$flipper_length_mm-fitted(fit))
## another option is 
head(fit$residuals)
```

L'assunzione di indipendenza è difficile da testare, ma dato che ogni osservazione è derivata da un pinguino diverso è probabile che le osservazioni siano indipendenti (ma cosa succede se per esempio c'è un effetto della colonia sulla relazione di interesse e noi campioniamo pinguini solo da una colonia?). 
Per verificare che la varianza sia costante possiamo invece guardare un grafico di $x_i$ VS $r_i$ e $\hat{y}_i$ VS $r_i$: 

```{r homeSched, fig.asp=0.6}
par(mfrow=c(1,2), pch=16,bty="l")
plot(pengs$body_mass_g, residuals(fit))
plot(fitted(fit), residuals(fit))
```

Non ci sono forti segnali di varianza non costante, ma è presente una qualche struttura nei grafici. 

R inoltre produce di default una serie di utili grafici basati sui residui quando si usa la funzione `plot` su un oggetto di classe `lm`: 

```{r residChecks}
par(mfrow=c(2,2))
plot(fit)
```

Vedremo più in dettaglio questi grafici nelle prossime lezioni.  

Sebbene sia stato mostrato come il modello di regressione lineare sia relativamente robusto ad alcune deviazioni dal modello - la stima dei parametri si basa sulle assunzioni specificate: se queste assunzioni non sono riscontrabili nei dati si corre il rischio di fare un inferenza non affidabile. è sempre raccomandabile verificare che le assunzioni del modello siano soddisfatte prima di utilizzare un modello per fare predizioni e prendere decisioni. 


# Verifiche della teoria tramite simulazione (Ultimi 15 min)

La simulazione è un approccio molto utile per indagare come si comportano i vari metodi di stima e per verificare in maniera empirica come si comportano gli stimatori. Il creare una procedura di simulazione dei dati per altro aiuta molto a capire in profondità quali sia il processo sotteso alla generazione dei dati che viene ipotizzato dal modello. 

Valutiamo quindi alcune proprietà degli stimatori tramite la simulazione di dati le cui caratteristiche possiamo controllare, per esmepio valutiamo la proprietà di non-distorsione e le formule derivate per la varianza degli stimatori:  

```{r}
set.seed(324) # per riproducibilità 
n <- 100 
x <- runif(n, -1, 1)
## le x sono fissate
## i veri valori dei coefficienti del modello 
b0 <- 1; b1 <- 2

# una possibile realizzazione del modello 
epsilon <- rexp(n,1)-1 ## errori a media 0 
# alcune alternative 
# epsilon <- runif(n,-1,1) ## errori a media 0 
# epsilon <- runif(n,-3,3) ## errori a media 0 
# epsilon <- rgamma(n,shape = 4, scale = 2)-8  ## errori a media 0 
# epsilon <- rexp(n,4)-1/4 ## errori a media 0 
# epsilon <- rnorm(n,0,2) ## errori a media 0 
y <- b0 + b1 * x + epsilon
coef(lm(y~x)) # close enough 
## do this 1000 time 
generate_get_betas <- function(bs, epsilonpar, x){
  ytemp <- b0 + b1 * x + (rexp(length(x),epsilonpar)-1/epsilonpar)
  lm(ytemp~x)$coef
}
generate_get_betas(bs= c(b0,b1), epsilonpar = 1, x = x)
out <- replicate(1000, generate_get_betas(bs= c(b0,b1), epsilonpar = 1, x = x))
par(mfrow=c(1,2))
hist(out[1,]);hist(out[2,])
# che forma hanno questi istogrammi? 
# unbiased 
rowMeans(out)
# variability
apply(out,1,sd)
se <- 1 # V[Y] = 1/lambda^2, when Y ~ exp(1)
s2x <- sum((x-mean(x))^2)/n
c(se * sqrt(1/n + mean(x)^2/(n * s2x)), se * sqrt(1/(n * s2x)))

## change the variability of the error 
out2 <- replicate(1000, generate_get_betas(bs= c(b0,b1), epsilonpar = 0.45, x = x))
apply(out2,1,sd)
# still unbiased 
apply(out2,1,mean)
```





