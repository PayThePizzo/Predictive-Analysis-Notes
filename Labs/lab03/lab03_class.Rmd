---
title: "Lab 03 - Multiple Linear Regression, model assessment"
output:
  html_document:
    theme: readable
    toc: yes
    code_folding: show
---


# The data 

Usiamo i dati di automobili visti nel lab precedente: leggiamo direttamente i dati usando il file in Moodle: 

```{r}
autompg <- read.csv(file = "autompg.csv")
```

Abbiamo 7 variabili: una variabile risposta (`mpg`) e 6 predittori: 

```{r dataplot}
## use rgb to define proportion of reg-green-blue in a color
## extra 4th argument is alpha - the transparency of the points 
par(col= rgb(0.4,0.4,0.9,0.8),pch=16)
plot(autompg)
```

Presi individualemnte alcuni predittori sembrano essere più o meno correlati con la variabile risposta:

```{r}
signif(cor(autompg),3)
```

Dovremo individuare un sottoinsieme di predittori utili a costruire un modello predittivo per `mpg` (vediamo inoltre che i predittori sono anche correlati tra loro). 

# Specificazione del modello 

Specifichiamo un modello con due predittori `hp` e `year`: 

\[
Y_i = \beta_0 + \beta_{hp} \text{hp}_{i} + \beta_{year} \text{year}_{i} + \epsilon_i, \qquad i = 1, 2, \ldots, n
\]
con $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$ (errori indipendenti). 

# Significatvita' del modello

La prima domanda a cui vogliamo dare risposta e' se questo modello e' significativo, ovvero che e' significativo contro il modello nullo. Formalmente dovremo dire che possiamo rifiutare, ad un livello di significativita' del 5%, che i coefficienti del modello siano pari a 0.

```{r, class.source = "fold-show"}
# Full model or alternative model
fit1 <- lm(mpg~hp+year, data = autompg)
# Null model
fit_null <- lm(mpg~1, data = autompg)
```

```{r}
summary(fit1)
```

$$H_0: \beta_{hp} =  \beta_{year} = 0$$
Quello che calcoliamo e'la statistica F, ovvero data $H_{0}$, quanto sarebbe possibile aver trovato una $SS_{RES}$ pari a quella fornita da $H_{1}$.

Alternativamente possiamo trovarla con un ANOVA del modello piu' semplice contro quello piu' complesso.

```{r}
anova(fit_null, fit1)
```
Da dove viene questo risultato?
Noi valutiamo la misura di bonta' d'adattamento dei modelli, la $SS_{RES}$, la porzione di variabilita' spiegata dal modello.

Nel modello nullo, la stima e' costante (si usa uno scalare per stimare y), questo porta ad una minore variabilita' spiegata dal modello nullo contro una grande variabilita' non spiegata affatto. Da qui notiamo che la `RSS` della riga 1, ovvero del modello nullo, e' molto piu' grande di quella della riga 2.

In poche parole, e' evidente che il modello nullo sia meno efficiente nella spiegazione della variabilita', anche perche' la F-statistic e' molto grande ed il p-value e' estremamente piccolo!

Il calcolo sottostante e' il seguente:

\[ F = \frac{(SS_{RES}(H_{0}) - SS_{RES}(H_{A}))/(p-q)}{SS_{RES}(H_{A})/(n-p)} = \frac {\sum_{i=1}^{n}(\hat{y_{A,i}}-\hat{y_{0,i}})^{2}/(p-q)}{\sum_{i=1}^{n}(y_{i}-\hat{y_{A,i}})^2/(n-p)} \thicksim F_{p-q, n-p} \]

Dove i gradi di liberta' pesano sia il nominatore che il denominatore, ed il denominatore ha funzione di standardizzazione.

```{r}
# Manually this is what R does when computing the F statistics

# 1. Computes the RSS
RSS_h0 <- sum(residuals(fit_null)^2)
RSS_h1 <- sum(residuals(fit1)^2)

c(RSS_h0, RSS_h1)

# RSS_h0 - RSS_h1
RSS_diff <- sum(residuals(fit_null)^2) - sum(residuals(fit1)^2)
# Which is equal to the difference of the estimated values
# of the two models
est_diff <- sum((fitted(fit_null)-fitted(fit1))^2)

c(RSS_diff, est_diff)
```

Se non ci fosse molta differenza tra i due modelli, queste somme sarebbero molto piccole e H1 non sarebbe molto significativo.

```{r}
# F statistic

# It explains how much more complex the model is with respect to H0
df_res_diff <- fit_null$df.residual - fit1$df.residual
# Above the fraction we find
num <- RSS_diff/df_res_diff
# Below the fraction we find
den <- RSS_h1/fit1$df.residual

# F statistic
fobs1 <- num/den

c(anova(fit_null, fit1)$F[2], fobs1)
```
Calcoliamo ora il p-value, che e' il valore

```{r}
# pvalue 
pf(fobs1, df1 = df_res_diff, 
   df2 = fit1$df.residual, 
   lower.tail = FALSE)

# It is so small that we get 0 
1-pf(fobs1, df1 = (fit_null$df.residual - fit1$df.residual), 
   df2 = fit1$df.residual)
```

L'alternativa e' di confrontare fobs con fcrit specificando il livello di significativita'

```{r}
# 2%, so what is the quantile that leave 98% of the probability to the left
q<- qf(.98, df1 = df_res_diff, 
   df2 = fit1$df.residual)
q

curve(df(x,df1 = (fit_null$df.residual - fit1$df.residual), 
   df2 = fit1$df.residual), from = 0, to = 10)
abline(v=q, col= "red", lwd = 3, lty =2)
#Too big to appear
points(fobs1, 0 , col = 2, pch = 4)
```
Abbiamo una fortissima evidenza che i nostri due parametri beta siano diversi da 0. Infatti:

```{r}
summary(fit1)$fstatistic
```

---

# Nested Models

Come possiamo confrontare due modelli che sono annidati?

```{r}
fit_base <- lm(mpg~wt+year, data = autompg)

# lm(target ~ ., data = ..) includes all features
fit_all <- lm(mpg~., data = autompg)

summary(fit_all)
```

Il p-value e' molto significativo

Questa diminuzione della stima di $\sigma^{2}$, ovvero $SS_{RES}$, e' diversa dal punto di vista statistico oppure stiamo stimando una porzione simile di variabilita'? Bisogna ricordare che il modello piu' complesso implica maggiore difficolta' con la predizione.

```{r}
# The SS Res diminishes
# 26 points more, but is it worth the complexity?
c(sum(residuals(fit_base)^2), sum(residuals(fit_all)^2))

```

Inoltre, il modello potrebbe essere sovraparametrizzato dal momento che molte variabili incluse nel secondo modello, non sono molto significative. Ad esempio `hp` che prima sembrava essere una variabile
interessante, ricopre ora un ruolo apparentemente marginale.

Dunque, ne vale la pena?

## Confidence Intervals

Creiamo un dataframe ad hoc per degli intervalli di confidenza e ne guardiamo l'ampiezza.

```{r}
# Create new dataframe, where for each of the predictors of mpg
# we take some data that is at extreme quantiles and at the median
# We apply a vectorized function
nd <- data.frame(apply(autompg, 2, quantile, c(0.1, 0.5, 0.9)))

# Confidence intervals for the two models
(ci_all <- predict(fit_all , newdata = nd, interval = "conf"))
(ci_base <- predict(fit_base , newdata = nd, interval = "conf"))

ci_all[,3]- ci_all[,2]

ci_base[,3]- ci_base[,2]
```

Si nota che gli intervalli di `ci_base` sono piu' stretti, R-squared e' piu' piccolo e che ha meno variabilita'nell'intervallo di stima che teniamo. 

Questo e' dovuto al fatto che nel modello sovraparametrizzato seguiamo delle caratteristiche peculiari dei dati che non necessariamente sono generalizzabili. Nel momento di valutazione di valutazione dell'errore di predizione, questo e' ben piu' grande!

Anche se qui e' effettivamente molto semplice scovare questi dettagli, non e' sempre cosi'.

## ANOVA

Andiamo quindi a confrontare se il miglioramento nell $R^{2}$ dei $SS_{RES}$ e' significativo.

```{r}
anova(fit_base, fit_all)
```
We can see that the F is not that big, nor the p-value is small.

```{r}
# By hand F-statistic verification
num <- (sum(residuals(fit_base)^2) - sum(residuals(fit_all)^2))/(fit_base$df.residual - fit_all$df.residual)
den <- sum(residuals(fit_all)^2)/fit_all$df.residual
fobs <- num/den

# p-value
p_val <- pf(fobs, lower.tail = FALSE, df1 = (fit_base$df.residual - fit_all$df.residual),
   df2 = fit_all$df.residual)

c(fobs, p_val)
c(anova(fit_base, fit_all)$F[2], anova(fit_base, fit_all)$Pr[2])
```


```{r}
# reject H0 if fobs > fcri
k <- qf(.98 , df1 = (fit_base$df.residual - fit_all$df.residual),
   df2 = fit_all$df.residual)
k

curve(df(x, df1 = (fit_base$df.residual - fit_all$df.residual),
   df2 = fit_all$df.residual), from = 0, to = 10)
abline(v=k, col= "red", lwd = 3, lty =2)
points(fobs, 0 , col = 2, pch = 4)
```

La statistica test non e' nella coda della distribuzione, quindi vi e' poca evidenza contro l'ipotesi nulla. Ci conviene mantenere il modello piu' parsimonioso e senza parametri che forniscano rumore.

---

# Bontà di adattemnto: confornto tra modelli non annidati 

Ora, dato che abbiamo modelli non annidati dobbiamo usare altre misure:

1. $Adjusted R^{2}$
2. ICs
    + AIC
    + BIC

## Adjusted R-squared

$$Adjusted \; R^{2} = 1 - \frac{SS_{RES}/(n-p-1)}{SS_{TOT}/(n-1)} $$

```{r}
# From model: r-square vs adj-r-square
c("R-square", summary(fit_base)$r.square)
c("Adjusted-r-squared", summary(fit_base)$adj.r.square)

# By hand
c(1-sum(residuals(fit_base)^2)/(sum((autompg$mpg - mean(autompg$mpg))^2)), 1-(sum(residuals(fit_base)^2)/fit_base$df.residual)/(sum((autompg$mpg - mean(autompg$mpg))^2)/fit_null$df.residual))

# Adjusted R Square for Fit
c( "Adjusted R Square for fit1", summary(fit1)$adj.r.square)
c( "Adjusted R Square for fit_base", summary(fit_base)$adj.r.square)
c( "Adjusted R Square for fit_all", summary(fit_all)$adj.r.square)
```


## Criteri di informazione 

### AIC

$$AIC(\mathcal{M}) = n * \log MSS_{RES}(\mathcal{M}) + 2p(\mathcal{M})$$

```{r}
# Log Likelihood
logLik(fit_base)
logLik(fit_all)

# By hand the logLik
sum(dnorm(autompg$mpg, mean = fitted(fit_base), 
           sd = summary(fit_base)$sigma, log = TRUE))

# AIC
c(AIC(fit_base), AIC(fit_all))

# By Hand
c(-2*logLik(fit_base)+2*(length(coef(fit_base))+1), -2*logLik(fit_all)+2*(length(coef(fit_all))+1))

```
Come vediamo, la differenza tra le logLik e' molto piccola ed aggiungere i 4 parametri in piu' non vale la candela.

Di default `AIC()` conta sigma come un parametro stimato, solo che l'ordine degli AIC non cambia!

### BIC
IC in cui la penalizzazione dei modelli complessi e' piu' ferrea.

$$BIC(\mathcal{M}) = n * \log MSS_{RES}(\mathcal{M}) + \log (n)p(\mathcal{M})$$

```{r}
# BIC
c(BIC(fit_base), BIC(fit_all))

# By hand
c(-2*logLik(fit_base)+log(nrow(autompg))*(length(coef(fit_base))+1), -2*logLik(fit_all)+log(nrow(autompg))*(length(coef(fit_all))+1))

# Which is the same as the AIC with log(n)
AIC(fit_base, k = log(nrow(autompg)))
```
Ed ancora, i pochi punti in piu' non sono sufficienti ad aumentare.

---

# Verificare la teoria tramite simulazione 
#TODO
Come interpretare l'effetto che hanno i singoli $\beta_{j}$...

```{r}
X <- model.matrix(fit_base) 
colnames(X) <- c("Int", "x1","x2")
sigma_true <- 3.4
beta_true <- c(-14, -0.006, .74)
n <- nrow(X)
# set.seed(8686)
y_alternative <- X %*% beta_true+  rnorm(n, 0, sigma_true)
autodf_alterntive <- data.frame(y = y_alternative, X[,-1])
lm(y~x1+x2, data = autodf_alterntive)
```

```{r}
generate_and_estimate <- function(X, truebeta, truesigma){
  n <- nrow(X)
  y_alternative <- X %*% truebeta+  rnorm(n, 0, truesigma)
  autodf_alterntive <- data.frame(y = y_alternative, X[,-1])
  estimate <- coef(lm(y~x1+x2, data = autodf_alterntive))
  estimate
}
generate_and_estimate(X = X, truebeta = beta_true, truesigma = sigma_true)
out <- replicate(n = 500, generate_and_estimate(X = X, truebeta = beta_true, truesigma = sigma_true))
hist(out[1,])
hist(out[2,])
hist(out[3,]); abline(v = beta_true[3], col = 2)
mean(out[3,])
```

```{r}
cov(t(out))
```

```{r}
sigma_true^2 * solve(t(X) %*%X)
```



