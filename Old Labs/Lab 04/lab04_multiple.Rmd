---
title: "Lab 04 - Multiple Linear Regression"
output:
  html_document:
    theme: readable
    toc: yes
    code_folding: show
---


# The data 

First off we read (and manipulate a bit) the data: 

```{r, class.source = "fold-hide"}
# read the data
fl <- "http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
autompg = read.table(fl, quote = "\"",
comment.char = "", stringsAsFactors = FALSE)
# give the dataframe headers
colnames(autompg) <- c("mpg", "cyl", "disp", "hp", "wt",
"acc", "year", "origin", "name")
# remove missing data, which is stored as "?"
autompg <-subset(autompg, autompg$hp != "?")
# remove the plymouth reliant, as it causes some issues
autompg <- subset(autompg, autompg$name != "plymouth reliant")
# give the dataset row names, based on the engine, year and name
rownames(autompg) <- paste(autompg$cyl, "cylinder",
autompg$year, autompg$name)
# remove the variable for name, as well as origin
autompg <- subset(autompg,
select = c("mpg", "cyl", "disp", "hp", "wt", "acc", "year"))
# change horsepower from character to numeric
autompg$hp <- as.numeric(autompg$hp)
```

For the moment we focus on explaining $mpg$ as a function of $wt$ and $year$.


```{r dataplot}
par(mfrow=c(1,2), bty="l",pch=16)
plot(mpg~wt, data= autompg)
plot(mpg~year, data= autompg)
```


# Model specification and estimation 

We start by extending the simple linear model to include more than one explanatory variable. In general we write: 
\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i, \qquad i = 1, 2, \ldots, n
\]
where $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ (and independent). 

In matrix form this corresponds to 

\[
\begin{bmatrix}
Y_1   \\
Y_2   \\
\vdots\\
Y_n   \\
\end{bmatrix}
=
\begin{bmatrix}
1      & x_{11}    & x_{12}    & \cdots & x_{1(p-1)} \\
1      & x_{21}    & x_{22}    & \cdots & x_{2(p-1)} \\
\vdots & \vdots    & \vdots    &  & \vdots \\
1      & x_{n1}    & x_{n2}    & \cdots & x_{n(p-1)} \\
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_{p-1} \\
\end{bmatrix}
+
\begin{bmatrix}
\epsilon_1   \\
\epsilon_2   \\
\vdots\\
\epsilon_n   \\
\end{bmatrix}
= X \beta + \epsilon, 
\]

where X is the so-called model matrix and $\beta$ is the parameter vector $(\beta_0, \beta_1, \ldots, \beta_{p-1})$. In this example we have taken $p=2$.  


For our application this translates to 
\[
\texttt{mpg}_i = \beta_0 + \beta_1 \texttt{wt}_{i} + \beta_2 \texttt{year}_{i} + \epsilon_i, \qquad i = 1, 2, \ldots, n
\]
where $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ (and independent). 

The model matrrix is then: 

\[ 
X = \begin{bmatrix}
1      & mpg_{1}     & year_{1} \\
1      & mpg_{2}     & year_{2} \\
\vdots & \vdots     & \vdots   \\
1      & mpg_{n}    & year_{n} \\
\end{bmatrix}
\]

To specify and estimate the model we use the `lm` function: 

```{r, class.source = "fold-show"}
fit <- lm(mpg~wt+year, data = autompg)
```

`fit` is now an object of class `lm` and we can interact with it using the functions we have already seen for the simple linear models 

```{r}
fit$coef; coef(fit)
head(fitted(fit))
head(residuals(fit))
```

# Inference on the parameters 

In particular we can use the `summary` function - which is very useful when we wish to do inference on the parameters of the model. 

```{r}
summary(fit)
```

Let's see where the numbers in the coefficients table come from. 

First we create the $X$ matrix: 

```{r}
n <- nrow(autompg)
X <- cbind(rep(1,n), autompg$wt, autompg$year)
y <- autompg$mpg
```

We have seen that the estimator for $\beta$ is: 
\[ \hat{\beta} = (X^T X) ^{-1} X^T y . \]

```{r}
(beta_hat <- as.vector(solve(t(X) %*% X) %*% t(X) %*% y))
coef(fit)
```

[Bonus question: is this really the least square estimate? You can write a function that compute the sum of squares for any vector of parameter $\beta$.]

Since we assume that the vector of observations is a realization of a multivariate normal distribution (with mean $X \beta$ and variance $\sigma^2 I_n$) we can show that $\beta$ also follows a multivariate normal distribution with: 

\[E[\hat{\beta}] = \beta \quad \text{and} \quad V[\hat{\beta}] = \sigma^2 (X^T X)^{-1}  \]

Since $\sigma^2$ is unknown we need to estimate it with: 
\[s^2_e = \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}{n-p} = \frac{\sum_{i=1}^{n}e^T e }{n-p} \]
where $e$ is the vector of the model errors. 

```{r}
(est_sigma <- sqrt(sum(fit$residuals^2)/(n-length(fit$coef))))
summary(fit)$sigma
(se_beta_hat <- as.vector(est_sigma * sqrt(diag(solve(t(X) %*% X)))))
summary(fit)$coef[,2]
```

Notice that we can actually look at the variance-covariance matrix of $\beta$: 

```{r}
(est_sigma^2) * (solve(t(X) %*% X))
vcov(fit)
```

Since covariances are not scaled one can also use the `cov2cor` function to assess the correlation between parameter estimates: 

```{r}
cov2cor(vcov(fit))
```

We would like to have small correlations- we will discuss more about this later. 

The last two columns of the `summary(fit)$coef` table are the test statistic for the two system of hypothesis: 
\[H_0: \beta_1 = 0 \quad VS \quad \beta_1 \neq 0 \]
\[H_0: \beta_2 = 0 \quad VS \quad \beta_2 \neq 0 \]
and their respective p-value. Notice these tests relate to the individual $\beta_j$ coefficient and are useful to assess whether the coefficient related to any of the $x_j$ variable can be deemed to be equal to 0. One might want to keep variables in the model even if the null hypothesis that the coefficient related to the variable is null can not be rejected. 

The test statistic values are obtained as: 
\[\frac{\hat{\beta}_j - 0}{se(\hat{\beta}_j)} \]

```{r tvals}
beta_hat/se_beta_hat
summary(fit)$coef[,3]
```

while confidence intervals can be obtained using `confint` (or by hand): 

```{r cis}
### 98% confidence intervals 
cbind(beta_hat+qt(0.01, n-length(fit$coefficients))*se_beta_hat,
      beta_hat+qt(0.99, n-length(fit$coefficients))*se_beta_hat)
confint(fit, level = .98)
```


To test whether there is evidence against $\beta_2 = \beta^*$, i.e. to test: 
\[H_0: \beta_2 = \beta^* \quad VS \quad \beta_2 \neq \beta^* \]
we use (taking $\beta^* = 0.86$): 

```{r}
(TS <- (beta_hat[3]-0.86)/se_beta_hat[3])
2*pt(abs(TS),n-2, lower.tail = FALSE)
## reject at 5%, do not reject at 2%
### one can also use the car::linearHypothesis function
``` 

Confidence intervals can be obtained in R directly using `confint` (as we did for the simple linear regression model): 

```{r}
confint(fit, level = 0.98) 
## livello di confidenza: 98%
cbind(beta_hat + qt(0.01, df=387) * se_beta_hat,
      beta_hat + qt(0.99, df=387) * se_beta_hat)
```

Finally we can use the function `confidenceEllipse` in the `car` package to draw bi-variate confidence intervals, here for $(\beta_0, \beta_1)$ and $\beta_0, \beta_2$:

```{r}
par(bty = "l", mfrow=c(1,2))
car::confidenceEllipse(fit, which.coef=c(1,2), vcov.=vcov(fit), grid = FALSE)
car::confidenceEllipse(fit, which.coef=c(1,3), vcov.=vcov(fit), grid = FALSE)
```

How is the ellipse built? As outlined in Lecture 18 of Cosma Shalizi's [lectures](https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures) we need to rely on the fact that the chi-square distribution is defined as the square of a normal distribution and the fact fact that sums of chi-squares distributions are chi-squared distributed. 
If we write $\mathbf{\beta}_q$ for the vector of coefficients we're interested in, and
$\mathbf{\Sigma}_q$ for its variance-covariance matrix, then the confidence
region is the set of all $\mathbf{\beta}_q$ where
\begin{equation*}
(\widehat{\mathbf{\beta}}_q - \mathbf{\beta}_q)^T \mathbf{\Sigma}^{-1}_q
(\widehat{\mathbf{\beta}}_q - \mathbf{\beta}_q) \leq \chi^2_q(1-\alpha)
\end{equation*}
The calculations needed to prove this are beyond this course. 

Using a the same results which underlie the construction of multivariate confidence intervals one can derive test for multiple values of the model coefficients using `linearHypothesis` in the `car` package (we do not do this in this course). 

# Inference on the estimated function 

The estimated values at the observed $x$ values are: 

\[\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{1i} + \ldots + \hat{\beta}_{(p-1)} x_{(p-1)i}  = X \hat{\beta} = X  (X^T X) ^{-1} X^T y. \]
Indeed the matrix $H = X  (X^T X) ^{-1} X^T$ is called the hat matrix, as it transforms the observed $y_i$ in the corresponding $\hat{y}_i$ (`y_hat`). 

```{r estiamtingY}
H <- X  %*% solve(t(X) %*% X) %*% t(X)
y_hat <- as.vector(H %*% y)
head(y_hat); head(as.vector(fit$fitted.values))
```

We can extract confidence and prediction intervals for the estimated function using `predict`: 

```{r predictFun}
nd <- data.frame(wt = c(1650, 3000, 5000), year = c(72, 75, 82))
(cint <- predict(fit, nd, interval = "conf"))
(pint <- predict(fit, nd, interval = "pred"))
```

The point estimates/the center of the interval are the same for prediction and confidence intervals, but the prediction intervals are wider. 

```{r}
cint[,3] - cint[,2]
pint[,3] - pint[,2]
```

Notice also some large differences in the width of the confidence interval - why is that?

```{r}
plot(autompg$wt, autompg$year,pch=16,col="dodgerblue2")
points(nd, pch =4, col="orange",lwd=3)
```

Less uncertainity for the point which is close to $(\bar{x}_1, \bar{x}_2)$. More uncertainity for the point which is far from the cloud of observed points. 


# Visualising the fit 

One we have estimated the values of $y_i$ - can we somehow visualize them? 

```{r fittedPlotWrong}
par(mfrow=c(1,2), bty="l",pch=16)
plot(mpg~wt, data= autompg,col="grey50")
lines(autompg$wt, y_hat, col = 4)
plot(mpg~year, data= autompg,col="grey50")
lines(autompg$year, y_hat, col = 2)
```

That looks pretty bad! 

The fitted values are obtained from the combination of the values of the two predictors: to make a "decent" plot we can not just use `abline` or plot the fitted value - need to fix the values of the other variable. We do this using `predict` or some other similar approach but keeping the value of the variable(s) not under consideration fixed. 
For example we can draw the line of how `wt` affects `mpg` for a car built in the mean year `r mean(autompg$year)`. Similarly, we can draw the effect of `year` on `mpg` for some fixed values of `wt`: 

```{r fittedPlot}
par(mfrow=c(1,2), bty="l",pch=16,col="grey50")
plot(mpg~wt, data= autompg)
nd <- data.frame(wt = seq(1500, 5200, length.out = 100), year = mean(autompg$year))
lines(nd$wt, predict(fit, nd), lwd=2,col=4)
plot(mpg~year, data= autompg)
## an "X" matrix but with one calumn with fixed values 
Xfix <- cbind(rep(1,n),rep(min(autompg$wt)),autompg$year)
lines(autompg$year, Xfix %*% coef(fit), lwd = 2 , col = 2)
Xfix <- cbind(rep(1,n),rep(median(autompg$wt)),autompg$year)
lines(autompg$year, Xfix %*% coef(fit), lwd = 2 , col = 5)
```

